[
  {
    "resourceId": 530479,
    "resourceName": "SDSC Expanse Projects Storage",
    "displayResourceName": "SDSC Expanse Projects Storage",
    "resourceType": "Storage",
    "description": "",
    "units": "GB",
    "organization": "San Diego Supercomputer Center",
    "resourceRepositoryKey": 145120,
    "productionBeginDate": "2020-10-01",
    "productionEndDate": "2025-10-01",
    "resourceDescription": "5PB of storage on a Lustre based filesystem.",
    "recommendedUse": "Use for storage needs of allocated projects on Expanse Compute and Expanse GPU resources. Unpurged storage available for duration of allocation period.",
    "conversionFactor": "8.0145",
    "userGuideUrl": "https://www.sdsc.edu/support/user_guides/expanse.html",
    "attributeSets": [],
    "featureCategories": [
      {
        "categoryId": 101,
        "categoryName": "Allocations",
        "categoryDescription": "Whether resource is allocated by ACCESS or not",
        "categoryIsFilter": true,
        "features": [
          {
            "featureId": 139,
            "name": "ACCESS Allocated",
            "description": "Resource is allocated by ACCESS",
            "isFilter": null
          }
        ]
      },
      {
        "categoryId": 100,
        "categoryName": "Resource Type",
        "categoryDescription": "The general kind of resource",
        "categoryIsFilter": true,
        "features": [
          {
            "featureId": 138,
            "name": "Storage",
            "description": "Storage is the main purpose of this resource",
            "isFilter": null
          }
        ]
      }
    ]
  },
  {
    "resourceId": 531298,
    "resourceName": "ACCESS Credits",
    "displayResourceName": "ACCESS Credits",
    "resourceType": "Program",
    "description": "",
    "units": "ACCESS Credits",
    "organization": "ACCESS Allocations",
    "resourceRepositoryKey": 145904,
    "productionBeginDate": "2022-09-01",
    "productionEndDate": null,
    "resourceDescription": null,
    "recommendedUse": "Credits should be exchanged for ACCESS resources",
    "conversionFactor": "8.0145",
    "userGuideUrl": "https://allocations.access-ci.org",
    "attributeSets": [],
    "featureCategories": []
  },
  {
    "resourceId": 531464,
    "resourceName": "Georgia Tech GaTech Hive Cluster",
    "displayResourceName": "Georgia Tech GaTech Hive Cluster",
    "resourceType": "Compute",
    "description": "As part of the ACCESS ecosystem, a portion of the system is available for community use through the Hive Gateway (https://gateway.hive.pace.gatech.edu). The Hive gateway permits direct use of a variety of software for users with ACCESS logins. No allocation necessary! Also through its ACCESS participation, the Hive cluster is available to support existing or new gateways using the Airavata platform. Interested gateways should contact pace-support@oit.gatech.edu.",
    "units": "Core-hours",
    "organization": "Georgia Institute of Technology",
    "resourceRepositoryKey": 146103,
    "productionBeginDate": "2020-10-01",
    "productionEndDate": null,
    "resourceDescription": null,
    "recommendedUse": "As part of the ACCESS ecosystem, a portion of the system is available for community use through the Hive Gateway (https://gateway.hive.pace.gatech.edu). The Hive gateway permits direct use of a variety of software for users with ACCESS logins. No allocation necessary! Also through its ACCESS participation, the Hive cluster is available to support existing or new gateways using the Airavata platform. Interested gateways should contact pace-support@oit.gatech.edu.",
    "conversionFactor": null,
    "userGuideUrl": "https://docs.pace.gatech.edu/hiveGateway/gettingStarted/",
    "attributeSets": [],
    "featureCategories": [
      {
        "categoryId": 100,
        "categoryName": "Resource Type",
        "categoryDescription": "The general kind of resource",
        "categoryIsFilter": true,
        "features": [
          {
            "featureId": 136,
            "name": "CPU Compute",
            "description": "General compute use",
            "isFilter": null
          }
        ]
      },
      {
        "categoryId": 101,
        "categoryName": "Allocations",
        "categoryDescription": "Whether resource is allocated by ACCESS or not",
        "categoryIsFilter": true,
        "features": [
          {
            "featureId": 140,
            "name": "RP-managed process",
            "description": "Resource is not allocated by ACCESS",
            "isFilter": null
          }
        ]
      },
      {
        "categoryId": 103,
        "categoryName": "Specialized Support",
        "categoryDescription": "Special services offered by the resource",
        "categoryIsFilter": true,
        "features": [
          {
            "featureId": 167,
            "name": "ACCESS OnDemand",
            "description": "An intuitive, innovative, and interactive interface to remote computing resources",
            "isFilter": null
          },
          {
            "featureId": 146,
            "name": "Science Gateway support",
            "description": "Support for Science Gateways",
            "isFilter": null
          }
        ]
      }
    ]
  },
  {
    "resourceId": 531199,
    "resourceName": "IACS at Stony Brook Ookami",
    "displayResourceName": "IACS at Stony Brook Ookami",
    "resourceType": "Compute",
    "description": "1 SU = 1 node hour\r\nA node is allocated exclusively to a user. Node-sharing is not available.",
    "units": "SUs",
    "organization": "Institute for Advanced Computational Science at Stony Brook University",
    "resourceRepositoryKey": 145574,
    "productionBeginDate": "2022-06-16",
    "productionEndDate": "2025-09-30",
    "resourceDescription": "Ookami is a computer technology testbed supported by the National Science Foundation under grant OAC 1927880. It provides researchers with access to the A64FX processor developed by Riken and Fujitsu for the Japanese path to exascale computing and is deployed in the, until June 2022, fastest computer in the world, Fugaku. It is the first such computer outside of Japan. By focusing on crucial architectural details, the ARM-based, multi-core, 512-bit SIMD-vector processor with ultrahigh-bandwidth memory promises to retain familiar and successful programming models while achieving very high performance for a wide range of applications. While being very power-efficient it supports a wide range of data types and enables both HPC and big data applications.\r\n\r\nThe Ookami HPE (formerly Cray) Apollo 80 system has 176 A64FX compute nodes each with 32GB of high-bandwidth memory and a 512 Gbyte SSD. This amounts to about 1.5M node hours per year. A high-performance Lustre filesystem provides about 0.8 Pbyte storage.\r\n\r\nTo facilitate users exploring current computer technologies and contrasting performance and programmability with the A64FX, Ookami also includes:\r\n\r\n- 1 node with dual socket AMD Milan (64cores) with 512 Gbyte memory\r\n- 2 nodes with dual socket Thunder X2 (64 cores) each with 256 Gbyte memory\r\n- 1 node with dual socket Intel Skylake (32 cores) with 192 Gbyte memory and 2 NVIDIA V100 GPUs",
    "recommendedUse": "Applications that are fitting within the memory requirements (27GB per node) and are well vectorized, or well auto-vectorized by the compiler.\r\nNote a node is allocated exclusively to one user. Node-sharing is not available.",
    "conversionFactor": "1.0",
    "userGuideUrl": "https://www.stonybrook.edu/commcms/ookami/support/faq/",
    "attributeSets": [],
    "featureCategories": [
      {
        "categoryId": 100,
        "categoryName": "Resource Type",
        "categoryDescription": "The general kind of resource",
        "categoryIsFilter": true,
        "features": [
          {
            "featureId": 136,
            "name": "CPU Compute",
            "description": "General compute use",
            "isFilter": null
          },
          {
            "featureId": 135,
            "name": "Innovative / Novel Compute",
            "description": "Unique, innovative or non-traditional compute resource",
            "isFilter": null
          }
        ]
      },
      {
        "categoryId": 101,
        "categoryName": "Allocations",
        "categoryDescription": "Whether resource is allocated by ACCESS or not",
        "categoryIsFilter": true,
        "features": [
          {
            "featureId": 139,
            "name": "ACCESS Allocated",
            "description": "Resource is allocated by ACCESS",
            "isFilter": null
          }
        ]
      },
      {
        "categoryId": 103,
        "categoryName": "Specialized Support",
        "categoryDescription": "Special services offered by the resource",
        "categoryIsFilter": true,
        "features": [
          {
            "featureId": 146,
            "name": "Science Gateway support",
            "description": "Support for Science Gateways",
            "isFilter": null
          }
        ]
      }
    ]
  },
  {
    "resourceId": 530808,
    "resourceName": "Indiana Jetstream2 Storage",
    "displayResourceName": "Indiana Jetstream2 Storage",
    "resourceType": "Storage",
    "description": "",
    "units": "GB",
    "organization": "Indiana University",
    "resourceRepositoryKey": 145449,
    "productionBeginDate": "2021-10-01",
    "productionEndDate": "2025-09-30",
    "resourceDescription": null,
    "recommendedUse": null,
    "conversionFactor": "8.0145",
    "userGuideUrl": "https://docs.jetstream-cloud.org/",
    "attributeSets": [],
    "featureCategories": [
      {
        "categoryId": 101,
        "categoryName": "Allocations",
        "categoryDescription": "Whether resource is allocated by ACCESS or not",
        "categoryIsFilter": true,
        "features": [
          {
            "featureId": 139,
            "name": "ACCESS Allocated",
            "description": "Resource is allocated by ACCESS",
            "isFilter": null
          }
        ]
      },
      {
        "categoryId": 100,
        "categoryName": "Resource Type",
        "categoryDescription": "The general kind of resource",
        "categoryIsFilter": true,
        "features": [
          {
            "featureId": 138,
            "name": "Storage",
            "description": "Storage is the main purpose of this resource",
            "isFilter": null
          },
          {
            "featureId": 133,
            "name": "Cloud",
            "description": "Cloud Resource",
            "isFilter": null
          }
        ]
      }
    ]
  },
  {
    "resourceId": 530639,
    "resourceName": "UD DARWIN GPU Nodes (DARWIN GPU)",
    "displayResourceName": "UD DARWIN GPU Nodes (DARWIN GPU)",
    "resourceType": "Compute",
    "description": "1 SU = 1 device hour;  \r\nT4 or MI50 device hour = 1 GPU + 64 CPU cores + 512 GiB RAM;  \r\nV100 device hour = 1 GPU + 12 CPU cores + 192 GiB RAM",
    "units": "SUs",
    "organization": "University of Delaware",
    "resourceRepositoryKey": 145278,
    "productionBeginDate": "2021-08-02",
    "productionEndDate": "2024-09-30",
    "resourceDescription": "3 nodes with two Intel® Xeon® Platinum 8260 processors (24 cores each), 768 GiB RAM, and 4 NVIDIA Tesla V100 32GB GPUs connected via NVLINK™\r\n9 nodes with two AMD EPYC™ 7502 processors (32 cores each), 512 GiB RAM, and a single NVIDIA Tesla T4 GPU\r\n1 node with two AMD EPYC™ 7502 processors (32 cores each), 512 GiB RAM, and a single AMD Radeon Instinct MI50 GPU",
    "recommendedUse": "DARWIN's GPU nodes provide resources for machine learning, artificial intelligence research,  and visualization.",
    "conversionFactor": "310.0",
    "userGuideUrl": "https://docs.hpc.udel.edu/abstract/darwin/darwin#compute-nodes",
    "attributeSets": [],
    "featureCategories": []
  },
  {
    "resourceId": 530640,
    "resourceName": "UD DARWIN Storage (DARWIN Storage)",
    "displayResourceName": "UD DARWIN Storage (DARWIN Storage)",
    "resourceType": "Storage",
    "description": "",
    "units": "GB",
    "organization": "University of Delaware",
    "resourceRepositoryKey": 145279,
    "productionBeginDate": "2021-08-02",
    "productionEndDate": "2024-09-30",
    "resourceDescription": "DARWIN's Lustre file system is for use with the DARWIN Compute and GPU nodes.",
    "recommendedUse": "DARWIN's Lustre storage should be used for storing input files, supporting data files, work files, and output files associated with computational tasks run on the cluster.",
    "conversionFactor": "8.0145",
    "userGuideUrl": "https://docs.hpc.udel.edu/abstract/darwin/darwin#storage",
    "attributeSets": [],
    "featureCategories": [
      {
        "categoryId": 100,
        "categoryName": "Resource Type",
        "categoryDescription": "The general kind of resource",
        "categoryIsFilter": true,
        "features": [
          {
            "featureId": 138,
            "name": "Storage",
            "description": "Storage is the main purpose of this resource",
            "isFilter": null
          }
        ]
      }
    ]
  },
  {
    "resourceId": 530638,
    "resourceName": "UD DARWIN Compute Nodes (DARWIN)",
    "displayResourceName": "UD DARWIN Compute Nodes (DARWIN)",
    "resourceType": "Compute",
    "description": "1 SU = 1 compute hour (any node type);  \r\nstandard = 1 core + 8 GiB RAM;  \r\nlarge = 1 core + 16 GiB RAM;  \r\nxlarge = 1 core + 32 GiB RAM;  \r\nlg-swap is billed as the entire node at 64 SUs per hour = 64 cores + 1024 GiB RAM + 2.73 TiB Optane NVMe swap",
    "units": "SUs",
    "organization": "University of Delaware",
    "resourceRepositoryKey": 145277,
    "productionBeginDate": "2021-08-02",
    "productionEndDate": "2024-09-30",
    "resourceDescription": "Nodes with two AMD EPYC™ 7502 processors (32 cores each) with three memory size options:\r\n48x standard 512 GiB; \r\n32x large-memory 1024 GiB; \r\n11x xlarge-memory 2048 GiB; \r\n1x lg-swap 1024 GiB RAM + 2.73 TiB Intel Optane NVMe swap",
    "recommendedUse": "DARWIN's standard memory nodes provide powerful general-purpose computing, data analytics, and pre- and post-processing capabilities.  The large and xlarge memory nodes enable memory-intensive applications and workflows that do not have distributed-memory implementations.",
    "conversionFactor": "4.45",
    "userGuideUrl": "https://docs.hpc.udel.edu/abstract/darwin/darwin#compute-nodes",
    "attributeSets": [],
    "featureCategories": []
  },
  {
    "resourceId": 530162,
    "resourceName": "TACC Long-term tape Archival Storage (Ranch)",
    "displayResourceName": "TACC Long-term tape Archival Storage (Ranch)",
    "resourceType": "Storage",
    "description": "",
    "units": "GB",
    "organization": "Texas Advanced Computing Center",
    "resourceRepositoryKey": 142293,
    "productionBeginDate": "2013-03-15",
    "productionEndDate": null,
    "resourceDescription": "TACC's long-term mass storage solution, Ranch, is an Oracle® StorageTek Modular Library System. Ranch utilizes Oracle's Sun Storage Archive Manager Filesystem (SAM-FS) for migrating files to/from a tape archival system with a current offline storage capacity of 60 PB.\r\nRanch's disk cache is built on Oracle's ZFS 7240 and Dell MD3600i disk arrays containing approximately 640 TB of usable spinning disk storage. These disk arrays are controlled by a Dell R720  SAM-FS Metadata server which has 16 CPUs and 72 GB of RAM.\r\nTwo Oracle StorageTek SL8500 Automated Tape Libraries house all of the offline archival storage. Each SL8500 library can house up to 10,000 tapes with 64 tape drive slots. One SL8500 is currently populated with 10,000 T-10000B media where each tape is capable of holding one TB of uncompressed data while the second SL8500 houses 6,000 of the latest T-10000C media which can hold five TB of uncompressed data. Each SL8500 library also contains eight handbots to manage tapes and move them to/from the tape drives with a pass-through port connecting the two SL8500 libraries. If necessary, up to four SL8500 libraries can be integrated into a single archival solution, allowing for an offline storage capacity of 200 PB with current tape media.",
    "recommendedUse": "TACC's High Performance Computing systems are used primarily for scientific computing with users having access to WORK, SCRATCH, and HOME file systems that are limited in size. This is also true for TACC's visualization system, Longhorn. The Ranch system serves the HPC and Vis community systems by providing a massive, high-performance file system for archival purposes. Space on Ranch can also be requested independent of an accompanying allocation on an XSEDE compute or visualization resource.\r\nPlease note that Ranch is an archival system. The ranch system is not backed up or replicated. This means that Ranch contains a single copy, and only a single copy, of your file/s. While lost data due to tape damage is rare, please keep this fact in mind for your data management plans.",
    "conversionFactor": "8.0145",
    "userGuideUrl": "https://portal.xsede.org/tacc-ranch",
    "attributeSets": [],
    "featureCategories": [
      {
        "categoryId": 101,
        "categoryName": "Allocations",
        "categoryDescription": "Whether resource is allocated by ACCESS or not",
        "categoryIsFilter": true,
        "features": [
          {
            "featureId": 139,
            "name": "ACCESS Allocated",
            "description": "Resource is allocated by ACCESS",
            "isFilter": null
          }
        ]
      },
      {
        "categoryId": 100,
        "categoryName": "Resource Type",
        "categoryDescription": "The general kind of resource",
        "categoryIsFilter": true,
        "features": [
          {
            "featureId": 138,
            "name": "Storage",
            "description": "Storage is the main purpose of this resource",
            "isFilter": null
          }
        ]
      }
    ]
  },
  {
    "resourceId": 531365,
    "resourceName": "MATCHPremier",
    "displayResourceName": "MATCHPremier",
    "resourceType": "Program",
    "description": "The pilot program for the ACCESS MATCHPremier embedded support service provides matches your project with an experienced research professional, identified by ACCESS Support to provide the skills needed to help your project, for example, with managing massive allocations or using ACCESS resources in novel ways. MATCHPremier consultants are selected from the Computational Science and Support Network (CSSN) — they may be facilitators, research software engineers, or other types of appropriate support personnel. MATCHPremier should be requested at least six months in advance of the anticipated need.For more information visit the ACCESS Support portal: https://support.access-ci.org/matchpremier\r\n\r\n",
    "units": "Yes / No",
    "organization": "ACCESS Support",
    "resourceRepositoryKey": 146004,
    "productionBeginDate": "2022-10-24",
    "productionEndDate": null,
    "resourceDescription": null,
    "recommendedUse": "MATCHPremier identifies consultants who are available to be embedded on your research team for 12- 8 months. Consultants have the expertise to help manage massive allocation plans or to use ACCESS resources in novel ways. Engagements should be requested at least six months in advance and are funded through your research award.",
    "conversionFactor": null,
    "userGuideUrl": "https://support.access-ci.org/matchpremier",
    "attributeSets": [],
    "featureCategories": [
      {
        "categoryId": 100,
        "categoryName": "Resource Type",
        "categoryDescription": "The general kind of resource",
        "categoryIsFilter": true,
        "features": [
          {
            "featureId": 137,
            "name": "Service / Other",
            "description": "An allocated resource that's not compute or storage",
            "isFilter": null
          }
        ]
      },
      {
        "categoryId": 101,
        "categoryName": "Allocations",
        "categoryDescription": "Whether resource is allocated by ACCESS or not",
        "categoryIsFilter": true,
        "features": [
          {
            "featureId": 139,
            "name": "ACCESS Allocated",
            "description": "Resource is allocated by ACCESS",
            "isFilter": null
          }
        ]
      }
    ]
  },
  {
    "resourceId": 530772,
    "resourceName": "Purdue Anvil GPU",
    "displayResourceName": "Purdue Anvil GPU",
    "resourceType": "Compute",
    "description": "1 Anvil GPU Service Unit = 1 A100 GPU hour on a quad-NVIDIA A100 node",
    "units": "SUs",
    "organization": "Purdue University",
    "resourceRepositoryKey": 145412,
    "productionBeginDate": "2021-10-01",
    "productionEndDate": "2026-10-01",
    "resourceDescription": "16 nodes each with four NVIDIA A100 Tensor Core GPUs providing 1.5 PF of single-precision performance to support machine learning and artificial intelligence applications.",
    "recommendedUse": "Anvil’s GPU nodes can be used for various Machine learning (ML), artificial intelligence (AI), and other GPU-enabled workloads. A comprehensive set of GPU software is available within Open OnDemand as well as the module system that includes GPU-enabled package builds and NVIDIA GPU Cloud (NGC) containers for common Python ML frameworks such as PyTorch, Keras, and TensorFlow. GPU-enabled scientific software supports a variety of disciplines and applications that include, but is not limited to, AMBER, LAMMPS, NAMD, Gromacs.",
    "conversionFactor": "539.4375",
    "userGuideUrl": "https://www.rcac.purdue.edu/anvil#docs",
    "attributeSets": [],
    "featureCategories": [
      {
        "categoryId": 103,
        "categoryName": "Specialized Support",
        "categoryDescription": "Special services offered by the resource",
        "categoryIsFilter": true,
        "features": [
          {
            "featureId": 166,
            "name": "ACCESS Pegasus",
            "description": "Run Jobs and Workflows on ACCESS Resources from a Single Entry Point",
            "isFilter": null
          },
          {
            "featureId": 167,
            "name": "ACCESS OnDemand",
            "description": "An intuitive, innovative, and interactive interface to remote computing resources",
            "isFilter": null
          },
          {
            "featureId": 146,
            "name": "Science Gateway support",
            "description": "Support for Science Gateways",
            "isFilter": null
          }
        ]
      },
      {
        "categoryId": 101,
        "categoryName": "Allocations",
        "categoryDescription": "Whether resource is allocated by ACCESS or not",
        "categoryIsFilter": true,
        "features": [
          {
            "featureId": 139,
            "name": "ACCESS Allocated",
            "description": "Resource is allocated by ACCESS",
            "isFilter": null
          }
        ]
      },
      {
        "categoryId": 100,
        "categoryName": "Resource Type",
        "categoryDescription": "The general kind of resource",
        "categoryIsFilter": true,
        "features": [
          {
            "featureId": 134,
            "name": "GPU Compute",
            "description": "GPU use is the main purpose of this resource",
            "isFilter": null
          }
        ]
      },
      {
        "categoryId": 133,
        "categoryName": "Resource Category",
        "categoryDescription": "NAIRR Pilot Resource Category",
        "categoryIsFilter": true,
        "features": [
          {
            "featureId": 232,
            "name": "Federal agency systems",
            "description": "Agency supercomputers and advanced architecture systems",
            "isFilter": null
          }
        ]
      }
    ]
  },
  {
    "resourceId": 530807,
    "resourceName": "Indiana Jetstream2 Large Memory",
    "displayResourceName": "Indiana Jetstream2 Large Memory",
    "resourceType": "Compute",
    "description": "Jetstream2 Large Memory VMs are charged 2x the number of vCPUs per hour because of the additional memory capacity. For instance, the r3.xl flavor that uses 128 cores costs 256 SUs per hour. VM sizes and cost per hour are available at the \u003ca href=\"https://docs.jetstream-cloud.org/general/vmsizes/\"\u003eJetstream website\u003c/a\u003e.",
    "units": "SUs",
    "organization": "Indiana University",
    "resourceRepositoryKey": 145448,
    "productionBeginDate": "2021-10-01",
    "productionEndDate": "2025-09-30",
    "resourceDescription": "Jetstream2 LM is a hybrid-cloud platform that provides flexible, on-demand, programmable cyberinfrastructure tools ranging from interactive virtual machine services to a variety of infrastructure and orchestration services for research and education. This particular portion of the resource is allocated separately from the primary resource and contains 32 nodes of GPU-ready 1TB RAM compute nodes, AMD Milan 7713 CPUs with 128 cores per node connected by 100gbps ethernet to the spine.",
    "recommendedUse": "For the researcher needing virtual machine services on demand as well as for software creators and researchers needing to create their own customized virtual machine environments. Additional use cases are for research-supporting infrastructure services that need to be \"always on\" as well as science gateway services and for education support, providing virtual machines for students. \r\n\r\nThis particular resource is for those required 512gb or 1tb for their workloads and must demonstrate that need.",
    "conversionFactor": "8.0145",
    "userGuideUrl": "https://docs.jetstream-cloud.org/",
    "attributeSets": [],
    "featureCategories": [
      {
        "categoryId": 102,
        "categoryName": "Specialized Hardware",
        "categoryDescription": "Key compute hardware components in this resource",
        "categoryIsFilter": true,
        "features": [
          {
            "featureId": 142,
            "name": "Large Memory Nodes",
            "description": "Specialized analysis nodes with large memory",
            "isFilter": null
          }
        ]
      },
      {
        "categoryId": 103,
        "categoryName": "Specialized Support",
        "categoryDescription": "Special services offered by the resource",
        "categoryIsFilter": true,
        "features": [
          {
            "featureId": 166,
            "name": "ACCESS Pegasus",
            "description": "Run Jobs and Workflows on ACCESS Resources from a Single Entry Point",
            "isFilter": null
          }
        ]
      },
      {
        "categoryId": 101,
        "categoryName": "Allocations",
        "categoryDescription": "Whether resource is allocated by ACCESS or not",
        "categoryIsFilter": true,
        "features": [
          {
            "featureId": 139,
            "name": "ACCESS Allocated",
            "description": "Resource is allocated by ACCESS",
            "isFilter": null
          }
        ]
      },
      {
        "categoryId": 100,
        "categoryName": "Resource Type",
        "categoryDescription": "The general kind of resource",
        "categoryIsFilter": true,
        "features": [
          {
            "featureId": 133,
            "name": "Cloud",
            "description": "Cloud Resource",
            "isFilter": null
          }
        ]
      }
    ]
  },
  {
    "resourceId": 530482,
    "resourceName": "PSC Bridges-2 Storage (PSC Ocean)",
    "displayResourceName": "PSC Bridges-2 Storage (PSC Ocean)",
    "resourceType": "Storage",
    "description": "Storage",
    "units": "GB",
    "organization": "Pittsburgh Supercomputing Center",
    "resourceRepositoryKey": 145122,
    "productionBeginDate": "2021-02-11",
    "productionEndDate": "2025-09-30",
    "resourceDescription": "The Bridges-2 Ocean data management system provides a unified, high-performance filesystem for active project data, archive, and resilience. Ocean consists of two tiers, disk and tape, transparently managed by HPE DMF as a single, highly usable namespace.\r\n\r\nOcean's disk subsystem, for active project data, is a high-performance, internally resilient Lustre parallel filesystem with 15PB of usable capacity, configured to deliver up to 129GB/s and 142GB/s of read and write bandwidth, respectively.\r\n\r\nOcean's tape subsystem, for archive and additional resilience, is a high-performance tape library with 7.2PB of uncompressed capacity (estimated 8.6PB compressed, with compression done transparently in hardware with no performance overhead), configured to deliver 50TB/hour.",
    "recommendedUse": "The Bridges-2 Ocean data management system provides high-performance and highly usable access to project and archive data. It is equally accessible to all Bridges-2 compute nodes, allowing seamless execution of data-intensive workflows involving components running on different compute resource types.",
    "conversionFactor": "80.145",
    "userGuideUrl": "http://www.psc.edu/resources/bridges-2/user-guide",
    "attributeSets": [],
    "featureCategories": [
      {
        "categoryId": 101,
        "categoryName": "Allocations",
        "categoryDescription": "Whether resource is allocated by ACCESS or not",
        "categoryIsFilter": true,
        "features": [
          {
            "featureId": 139,
            "name": "ACCESS Allocated",
            "description": "Resource is allocated by ACCESS",
            "isFilter": null
          }
        ]
      },
      {
        "categoryId": 100,
        "categoryName": "Resource Type",
        "categoryDescription": "The general kind of resource",
        "categoryIsFilter": true,
        "features": [
          {
            "featureId": 138,
            "name": "Storage",
            "description": "Storage is the main purpose of this resource",
            "isFilter": null
          }
        ]
      }
    ]
  },
  {
    "resourceId": 530374,
    "resourceName": "PSC Anton 2 Special-Purpose Supercomputer for Molecular Dynamics (PSC Anton 2)",
    "displayResourceName": "PSC Anton 2 Special-Purpose Supercomputer for Molecular Dynamics (PSC Anton 2)",
    "resourceType": "Compute",
    "description": "Anton 2 is allocated annually via a Request for Proposal with proposals reviewed by a committee convened by the National Research Council at the National Academies. To qualify for an allocation on Anton 2, the principal investigator must be a faculty or staff member at a U.S. academic or non-profit research institution. Anton is allocated in MD simulation units.",
    "units": "MD Simulation Units",
    "organization": "Pittsburgh Supercomputing Center",
    "resourceRepositoryKey": 144815,
    "productionBeginDate": "2023-02-01",
    "productionEndDate": null,
    "resourceDescription": "Anton is a special purpose supercomputer for biomolecular simulation designed and constructed by D. E. Shaw Research (DESRES).   PSC's current system is known as Anton 2 and is a successor to the original Anton 1 machine hosted here.  \r\n\r\nAnton 2, the next-generation Anton supercomputer, is a 128 node system, made available without cost by DESRES for non-commercial research use by US universities and other not-for-profit institutions, and is hosted by PSC with support from the NIH National Institute of General Medical Sciences.  It replaced the original Anton 1 system in the fall of 2016.\r\n\r\nAnton was designed to dramatically increase the speed of molecular dynamics (MD) simulations compared with the previous state of the art, allowing biomedical researchers to understand the motions and interactions of proteins and other biologically important molecules over much longer time periods than was previously accessible to computational study.  The MD research community is using the Anton 2 machine at PSC to investigate important biological phenomena that due to their intrinsically long time scales have been outside the reach of even the most powerful general-purpose scientific computers.  Application areas include biomolecular energy transformation, ion channel selectivity and gating, drug interactions with proteins and nucleic acids, protein folding and protein-membrane signaling.",
    "recommendedUse": "As part of the ACCESS ecosystem, Anton 2 invites interested molecular dynamics (MD) investigators to request allocations on the special purpose supercomputer built specifically for MD. Requests for proposals are accepted each summer. The next request period will be in 2024. For detailed application instructions please see (https://www.psc.edu/resources/anton-2/anton-rfp/). Questions regarding Anton 2 can be directed to: grants@psc.edu.",
    "conversionFactor": null,
    "userGuideUrl": "https://www.psc.edu/resources/anton-2/",
    "attributeSets": [],
    "featureCategories": [
      {
        "categoryId": 100,
        "categoryName": "Resource Type",
        "categoryDescription": "The general kind of resource",
        "categoryIsFilter": true,
        "features": [
          {
            "featureId": 135,
            "name": "Innovative / Novel Compute",
            "description": "Unique, innovative or non-traditional compute resource",
            "isFilter": null
          }
        ]
      },
      {
        "categoryId": 101,
        "categoryName": "Allocations",
        "categoryDescription": "Whether resource is allocated by ACCESS or not",
        "categoryIsFilter": true,
        "features": [
          {
            "featureId": 140,
            "name": "RP-managed process",
            "description": "Resource is not allocated by ACCESS",
            "isFilter": null
          }
        ]
      }
    ]
  },
  {
    "resourceId": 530157,
    "resourceName": "OSG Open Science Pool",
    "displayResourceName": "OSG Open Science Pool",
    "resourceType": "Compute",
    "description": "1 SU = 1 Core-hour",
    "units": "SUs",
    "organization": "Open Science Grid",
    "resourceRepositoryKey": 142288,
    "productionBeginDate": "2012-04-01",
    "productionEndDate": "2025-12-31",
    "resourceDescription": "A virtual HTCondor pool made up of resources from the Open Science Grid",
    "recommendedUse": "High throughput jobs using a single core, or a small number of threads which can fit on a single compute node.",
    "conversionFactor": "3.147",
    "userGuideUrl": "https://portal.osg-htc.org/documentation/",
    "attributeSets": [],
    "featureCategories": [
      {
        "categoryId": 101,
        "categoryName": "Allocations",
        "categoryDescription": "Whether resource is allocated by ACCESS or not",
        "categoryIsFilter": true,
        "features": [
          {
            "featureId": 139,
            "name": "ACCESS Allocated",
            "description": "Resource is allocated by ACCESS",
            "isFilter": null
          }
        ]
      },
      {
        "categoryId": 103,
        "categoryName": "Specialized Support",
        "categoryDescription": "Special services offered by the resource",
        "categoryIsFilter": true,
        "features": [
          {
            "featureId": 145,
            "name": "Preemption",
            "description": "preemption",
            "isFilter": null
          },
          {
            "featureId": 146,
            "name": "Science Gateway support",
            "description": "Support for Science Gateways",
            "isFilter": null
          }
        ]
      }
    ]
  },
  {
    "resourceId": 531562,
    "resourceName": "TACC Dell/Intel Sapphire Rapids, Ice Lake, Skylake (Stampede3)",
    "displayResourceName": "TACC Dell/Intel Sapphire Rapids, Ice Lake, Skylake (Stampede3)",
    "resourceType": "Compute",
    "description": "DECEMBER 1, 2023 WILL BE THE START DATE FOR STAMPEDE3 MAXIMIZE ALLOCATIONS\r\n\r\nStampede3 is allocated in service units (SU)s. \r\nThe charge rate varies by node type:\r\n1) Skylake 1 node-hour = 1 SU\r\n2) Ice Lake 1 node-hour = 1.66 SU\r\n3) Sapphire Rapids  1 node-hour = 3 SU",
    "units": "Node Hours",
    "organization": "University of Texas at Austin",
    "resourceRepositoryKey": 146465,
    "productionBeginDate": "2024-04-27",
    "productionEndDate": "2028-09-30",
    "resourceDescription": "Stampede3 is generously funded through the National Science Foundation and is designed to serve today's researchers as well as support the research community on an evolutionary path toward many-core processors and accelerated technologies. Stampede 3 maintains the familiar programming model for all of today's users, and thus will be broadly useful for traditional simulation users, users performing data intensive computations, and emerging classes of new users.",
    "recommendedUse": "Stampede3 is intended primarily for parallel applications scalable to tens of thousands of cores, as well as general purpose and throughput computing. Normal batch queues will enable users to run simulations up to 48 hours.",
    "conversionFactor": "512.0",
    "userGuideUrl": "https://docs.tacc.utexas.edu/hpc/stampede3/",
    "attributeSets": [],
    "featureCategories": [
      {
        "categoryId": 100,
        "categoryName": "Resource Type",
        "categoryDescription": "The general kind of resource",
        "categoryIsFilter": true,
        "features": [
          {
            "featureId": 136,
            "name": "CPU Compute",
            "description": "General compute use",
            "isFilter": null
          },
          {
            "featureId": 134,
            "name": "GPU Compute",
            "description": "GPU use is the main purpose of this resource",
            "isFilter": null
          },
          {
            "featureId": 135,
            "name": "Innovative / Novel Compute",
            "description": "Unique, innovative or non-traditional compute resource",
            "isFilter": null
          }
        ]
      },
      {
        "categoryId": 101,
        "categoryName": "Allocations",
        "categoryDescription": "Whether resource is allocated by ACCESS or not",
        "categoryIsFilter": true,
        "features": [
          {
            "featureId": 139,
            "name": "ACCESS Allocated",
            "description": "Resource is allocated by ACCESS",
            "isFilter": null
          },
          {
            "featureId": 140,
            "name": "RP-managed process",
            "description": "Resource is not allocated by ACCESS",
            "isFilter": null
          }
        ]
      }
    ]
  },
  {
    "resourceId": 530641,
    "resourceName": "NCSA Delta CPU (Delta CPU)",
    "displayResourceName": "NCSA Delta CPU (Delta CPU)",
    "resourceType": "Compute",
    "description": "1 Core hour = 1 CPU Core-hour in the normal priority partition. One hour using a full Delta CPU node will cost 128 core hours in the normal priority partition.",
    "units": "Core-hours",
    "organization": "National Center for Supercomputing Applications",
    "resourceRepositoryKey": 145281,
    "productionBeginDate": "2022-09-19",
    "productionEndDate": "2027-08-31",
    "resourceDescription": "The Delta CPU resource comprises 124 dual-socket compute nodes for general purpose computation across a broad range of domains able to benefit from the scalar and multi-core performance provided by the CPUs, such as appropriately scaled weather and climate, hydrodynamics, astrophysics, and engineering modeling and simulation, and other domains using algorithms not yet adapted for the GPU. Each Delta CPU node is configured with 2 AMD EPYC 7763 (“Milan”) processors with 64-cores/socket (128-cores/node) at 2.45GHz and 256GB of DDR4-3200 RAM. An 800GB, NVMe solid-state disk is available for use as local scratch space during job execution. All Delta CPU compute nodes are interconnected to each other and to the Delta storage resource by a 100 Gb/sec HPE Slingshot network fabric.",
    "recommendedUse": "The Delta CPU resource is designed for general purpose computation across a broad range of domains able to benefit from the scalar and multi-core performance provided by the CPUs such as appropriately scaled weather and climate, hydrodynamics, astrophysics, and engineering modeling and simulation, and other domains that have algorithms that have not yet moved to the GPU. Delta also supports domains that employ data analysis, data analytics, or other data-centric methods. Delta features a rich base of preinstalled applications, based on user demand.  The system is optimized for capacity computing, with rapid turnaround for small to modest scale jobs, and features support for shared-node usage. Local SSD storage on each compute node benefits applications with random access data patterns or require fast access to significant amounts of compute-node local scratch space.\r\nThis allocation type is specific to and required for the CPU-only nodes on Delta. Request this allocation type if you have CPU only parts to your workflow.",
    "conversionFactor": "8.015",
    "userGuideUrl": "https://docs.ncsa.illinois.edu/systems/delta/en/latest/index.html",
    "attributeSets": [],
    "featureCategories": [
      {
        "categoryId": 101,
        "categoryName": "Allocations",
        "categoryDescription": "Whether resource is allocated by ACCESS or not",
        "categoryIsFilter": true,
        "features": [
          {
            "featureId": 139,
            "name": "ACCESS Allocated",
            "description": "Resource is allocated by ACCESS",
            "isFilter": null
          }
        ]
      },
      {
        "categoryId": 103,
        "categoryName": "Specialized Support",
        "categoryDescription": "Special services offered by the resource",
        "categoryIsFilter": true,
        "features": [
          {
            "featureId": 143,
            "name": "Advance reservations",
            "description": "has advance reservations",
            "isFilter": null
          },
          {
            "featureId": 146,
            "name": "Science Gateway support",
            "description": "Support for Science Gateways",
            "isFilter": null
          },
          {
            "featureId": 167,
            "name": "ACCESS OnDemand",
            "description": "An intuitive, innovative, and interactive interface to remote computing resources",
            "isFilter": null
          }
        ]
      },
      {
        "categoryId": 100,
        "categoryName": "Resource Type",
        "categoryDescription": "The general kind of resource",
        "categoryIsFilter": true,
        "features": [
          {
            "featureId": 136,
            "name": "CPU Compute",
            "description": "General compute use",
            "isFilter": null
          }
        ]
      }
    ]
  },
  {
    "resourceId": 531826,
    "resourceName": "PSC Brain Image Library (BIL)",
    "displayResourceName": "PSC Brain Image Library (BIL)",
    "resourceType": "Storage",
    "description": "BIL provides rich neuroscience image data collected in alignment with FAIR (Findable, Accessible, interoperable, reusable)  principles. Experimentalists  can deposit BRAIN data originating from microscopy experiments for indexing and distribution to others. Researchers interested in using BIL data for re-analysis or other purposes (including algorithm development)   can request in-place access to datasets coupled with computing infrastructure provided by PSC Bridges-2 compute, GPU, and storage resources. Interested researchers are requested to contact PSC via email at bil-support@psc.edu",
    "units": "Yes / No",
    "organization": "Pittsburgh Supercomputing Center",
    "resourceRepositoryKey": 146668,
    "productionBeginDate": null,
    "productionEndDate": null,
    "resourceDescription": null,
    "recommendedUse": "For researchers and data generators in the field of neuroscience, BIL provides a centralized hub for depositing, accessing, and sharing large brain image microscopy datasets to foster data-driven collaboration across the broader research community. AI and machine learning developers can harness the rich datasets to build and optimize models to advance neuroscience and computational image analysis domains.",
    "conversionFactor": null,
    "userGuideUrl": null,
    "attributeSets": [],
    "featureCategories": [
      {
        "categoryId": 101,
        "categoryName": "Allocations",
        "categoryDescription": "Whether resource is allocated by ACCESS or not",
        "categoryIsFilter": true,
        "features": [
          {
            "featureId": 140,
            "name": "RP-managed process",
            "description": "Resource is not allocated by ACCESS",
            "isFilter": null
          }
        ]
      },
      {
        "categoryId": 100,
        "categoryName": "Resource Type",
        "categoryDescription": "The general kind of resource",
        "categoryIsFilter": true,
        "features": [
          {
            "featureId": 135,
            "name": "Innovative / Novel Compute",
            "description": "Unique, innovative or non-traditional compute resource",
            "isFilter": null
          }
        ]
      }
    ]
  },
  {
    "resourceId": 530642,
    "resourceName": "NCSA Delta Storage (Delta Storage)",
    "displayResourceName": "NCSA Delta Storage (Delta Storage)",
    "resourceType": "Storage",
    "description": "",
    "units": "GB",
    "organization": "National Center for Supercomputing Applications",
    "resourceRepositoryKey": 145283,
    "productionBeginDate": "2022-09-19",
    "productionEndDate": "2027-08-31",
    "resourceDescription": "The Delta Storage resource provides storage allocations for projects using the Delta CPU and Delta GPU resources. It delivers 7PB of capacity to projects on Delta and will be augmented by a later expansion of 3PB of flash capacity for high-speed, data-intensive workloads.",
    "recommendedUse": "The Delta Storage resource provides storage allocations on the scratch file system for allocated projects using the Delta CPU and Delta GPU resources. Allocations are available for the duration of the compute allocation period. Scratch is unpurged storage.",
    "conversionFactor": "8.0145",
    "userGuideUrl": "https://docs.ncsa.illinois.edu/systems/delta/en/latest/index.html",
    "attributeSets": [],
    "featureCategories": [
      {
        "categoryId": 101,
        "categoryName": "Allocations",
        "categoryDescription": "Whether resource is allocated by ACCESS or not",
        "categoryIsFilter": true,
        "features": [
          {
            "featureId": 139,
            "name": "ACCESS Allocated",
            "description": "Resource is allocated by ACCESS",
            "isFilter": null
          }
        ]
      },
      {
        "categoryId": 100,
        "categoryName": "Resource Type",
        "categoryDescription": "The general kind of resource",
        "categoryIsFilter": true,
        "features": [
          {
            "featureId": 138,
            "name": "Storage",
            "description": "Storage is the main purpose of this resource",
            "isFilter": null
          }
        ]
      }
    ]
  },
  {
    "resourceId": 530643,
    "resourceName": "NCSA Delta GPU (Delta GPU)",
    "displayResourceName": "NCSA Delta GPU (Delta GPU)",
    "resourceType": "Compute",
    "description": "1 GPU Hour = 1 A100 GPU-Hour on a quad-NVIDIA A100 node (using 1 of the 4 GPUs) in the normal queue. A40 GPUs are discounted while large memory GPU nodes have a premium charge. Details on the charge rates are available in the \u003ca href=\"https://docs.ncsa.illinois.edu/systems/delta/en/latest/\" target=\"_blank\"\u003eDelta user guide\u003c/a\u003e (refer to the partitions section under running jobs). ",
    "units": "GPU Hours",
    "organization": "National Center for Supercomputing Applications",
    "resourceRepositoryKey": 145282,
    "productionBeginDate": "2022-09-19",
    "productionEndDate": "2027-08-31",
    "resourceDescription": "The Delta GPU resource comprises 4 different node configurations intended to support accelerated computation across a broad range of domains such as soft-matter physics, molecular dynamics, replica-exchange molecular dynamics, machine learning, deep learning, natural language processing, textual analysis, visualization, ray tracing, and accelerated analysis of very large in-memory datasets. Delta is designed to support the transition of applications from CPU-only to using the GPU or hybrid CPU-GPU models. Delta GPU resource capacity is predominately provided by 200 single-socket nodes, each configured with 1 AMD EPYC 7763 (“Milan”) processors with 64-cores/socket (64-cores/node) at 2.55GHz and 256GB of DDR4-3200 RAM. Half of these single-socket GPU nodes (100 nodes) are configured with 4 NVIDIA A100 GPUs with 40GB HBM2 RAM and NVLink (400 total A100 GPUs); the remaining half (100 nodes) are configured with 4 NVIDIA A40 GPUs with 48GB GDDR6 RAM and PCIe 4.0 (400 total A40 GPUs). Rounding out the GPU resource is 6 additional “dense” GPU nodes, containing 8 GPUs each, in a dual-socket CPU configuration (128-cores per node) and 2TB of DDR4-3200 RAM but otherwise configured similarly to the single-socket GPU nodes. Within the “dense” GPU nodes, 5 nodes employ NVIDIA A100 GPUs (40 total A100 GPUs in “dense” configuration) and 1 node employs AMD MI100 GPUs (8 total MI100 GPUs) with 32GB HBM2 RAM. A 1.6TB, NVMe solid-state disk is available for use as local scratch space during job execution on each GPU node type. All Delta GPU compute nodes are interconnected to each other and to the Delta storage resource by a 100 Gb/sec HPE Slingshot network fabric.",
    "recommendedUse": "The Delta GPU resource is designed to support accelerated computation across a broad range of domains such as soft-matter physics, molecular dynamics, replica-exchange molecular dynamics, machine learning, deep learning, natural language processing, textual analysis, visualization, ray tracing, and accelerated analysis of very large in-memory datasets. Delta is designed to support the transition of applications from CPU-only to using the GPU or hybrid CPU-GPU models. Delta features a rich base of preinstalled applications, based on user demand. The system is optimized for capacity computing, with rapid turnaround for small to modest scale jobs, and features support for shared-node usage. Local SSD storage on each compute node benefits applications with random access data patterns or require fast access to significant amounts of compute-node local scratch space.\r\nThis allocation type covers the use of all Delta GPU node types and all resources (CPU and GPU) within those nodes. If you have CPU only portions of your workflow you should also request an allocation of time under NCSA Delta CPU which provides access to CPU-only compute nodes.",
    "conversionFactor": "534.3333",
    "userGuideUrl": "https://docs.ncsa.illinois.edu/systems/delta/en/latest/",
    "attributeSets": [],
    "featureCategories": [
      {
        "categoryId": 101,
        "categoryName": "Allocations",
        "categoryDescription": "Whether resource is allocated by ACCESS or not",
        "categoryIsFilter": true,
        "features": [
          {
            "featureId": 139,
            "name": "ACCESS Allocated",
            "description": "Resource is allocated by ACCESS",
            "isFilter": null
          }
        ]
      },
      {
        "categoryId": 103,
        "categoryName": "Specialized Support",
        "categoryDescription": "Special services offered by the resource",
        "categoryIsFilter": true,
        "features": [
          {
            "featureId": 143,
            "name": "Advance reservations",
            "description": "has advance reservations",
            "isFilter": null
          },
          {
            "featureId": 146,
            "name": "Science Gateway support",
            "description": "Support for Science Gateways",
            "isFilter": null
          },
          {
            "featureId": 167,
            "name": "ACCESS OnDemand",
            "description": "An intuitive, innovative, and interactive interface to remote computing resources",
            "isFilter": null
          }
        ]
      },
      {
        "categoryId": 102,
        "categoryName": "Specialized Hardware",
        "categoryDescription": "Key compute hardware components in this resource",
        "categoryIsFilter": true,
        "features": [
          {
            "featureId": 142,
            "name": "Large Memory Nodes",
            "description": "Specialized analysis nodes with large memory",
            "isFilter": null
          }
        ]
      },
      {
        "categoryId": 100,
        "categoryName": "Resource Type",
        "categoryDescription": "The general kind of resource",
        "categoryIsFilter": true,
        "features": [
          {
            "featureId": 136,
            "name": "CPU Compute",
            "description": "General compute use",
            "isFilter": null
          },
          {
            "featureId": 134,
            "name": "GPU Compute",
            "description": "GPU use is the main purpose of this resource",
            "isFilter": null
          }
        ]
      },
      {
        "categoryId": 133,
        "categoryName": "Resource Category",
        "categoryDescription": "NAIRR Pilot Resource Category",
        "categoryIsFilter": true,
        "features": [
          {
            "featureId": 232,
            "name": "Federal agency systems",
            "description": "Agency supercomputers and advanced architecture systems",
            "isFilter": null
          }
        ]
      }
    ]
  },
  {
    "resourceId": 530605,
    "resourceName": "Open Storage Network (OSN)",
    "displayResourceName": "Open Storage Network (OSN)",
    "resourceType": "Storage",
    "description": "Amount of storage capacity being requested, expressed in base 10 units. The minimum allocation is 10TB. Larger allocations of up to 300TB may be accommodated with further justification and approval by OSN. Storage access is via AWS S3-compatible tools such as rclone, cyberduck and the AWS command-line interface for S3. A web interface is provided at https://portal.osn.xsede.org for allocated project users to manage and browse their storage.",
    "units": "TB",
    "organization": "Open Storage Network",
    "resourceRepositoryKey": 145013,
    "productionBeginDate": "2021-04-01",
    "productionEndDate": null,
    "resourceDescription": "The Open Storage Network (OSN) is an NSF-funded cloud storage resource, geographically distributed among several pods. OSN pods are currently hosted at SDSC, NCSA, MGHPCC, RENCI, and Johns Hopkins University. Each OSN pod currently hosts 1PB of storage, and is connected to R\u0026E networks at 50 Gbps. OSN storage is allocated in buckets, and is accessed using S3 interfaces with tools like rclone, cyberduck, or the AWS cli.",
    "recommendedUse": "Cloud-style storage of project datasets for access using AWS S3-compatible tools. The minimum allocation is 10TB. Storage allocations up to 300TB may be requested via the XSEDE resource allocation process.",
    "conversionFactor": "8.0145",
    "userGuideUrl": "https://openstoragenetwork.readthedocs.io/en/latest/",
    "attributeSets": [],
    "featureCategories": [
      {
        "categoryId": 100,
        "categoryName": "Resource Type",
        "categoryDescription": "The general kind of resource",
        "categoryIsFilter": true,
        "features": [
          {
            "featureId": 138,
            "name": "Storage",
            "description": "Storage is the main purpose of this resource",
            "isFilter": null
          }
        ]
      },
      {
        "categoryId": 101,
        "categoryName": "Allocations",
        "categoryDescription": "Whether resource is allocated by ACCESS or not",
        "categoryIsFilter": true,
        "features": [
          {
            "featureId": 139,
            "name": "ACCESS Allocated",
            "description": "Resource is allocated by ACCESS",
            "isFilter": null
          }
        ]
      }
    ]
  },
  {
    "resourceId": 530805,
    "resourceName": "Indiana Jetstream2 CPU",
    "displayResourceName": "Indiana Jetstream2 CPU",
    "resourceType": "Compute",
    "description": "1 SU = 1 Jetstream2 vCPU-hour. VM sizes and cost per hour are available https://docs.jetstream-cloud.org/general/vmsizes/",
    "units": "SUs",
    "organization": "Indiana University",
    "resourceRepositoryKey": 145446,
    "productionBeginDate": "2022-09-07",
    "productionEndDate": "2027-09-30",
    "resourceDescription": "Jetstream2 is a hybrid-cloud platform that provides flexible, on-demand, programmable cyberinfrastructure tools ranging from interactive virtual machine services to a variety of infrastructure and orchestration services for research and education. The primary resource is a standard CPU resource consisting of AMD Milan 7713 CPUs with 128 cores per node and 512gb RAM per node connected by 100gbps ethernet to the spine.",
    "recommendedUse": "For the researcher needing virtual machine services on demand as well as for software creators and researchers needing to create their own customized virtual machine environments. Additional use cases are for research-supporting infrastructure services that need to be \"always on\" as well as science gateway services and for education support, providing virtual machines for students.",
    "conversionFactor": "8.0145",
    "userGuideUrl": "https://docs.jetstream-cloud.org/",
    "attributeSets": [],
    "featureCategories": [
      {
        "categoryId": 103,
        "categoryName": "Specialized Support",
        "categoryDescription": "Special services offered by the resource",
        "categoryIsFilter": true,
        "features": [
          {
            "featureId": 166,
            "name": "ACCESS Pegasus",
            "description": "Run Jobs and Workflows on ACCESS Resources from a Single Entry Point",
            "isFilter": null
          },
          {
            "featureId": 146,
            "name": "Science Gateway support",
            "description": "Support for Science Gateways",
            "isFilter": null
          }
        ]
      },
      {
        "categoryId": 101,
        "categoryName": "Allocations",
        "categoryDescription": "Whether resource is allocated by ACCESS or not",
        "categoryIsFilter": true,
        "features": [
          {
            "featureId": 139,
            "name": "ACCESS Allocated",
            "description": "Resource is allocated by ACCESS",
            "isFilter": null
          }
        ]
      },
      {
        "categoryId": 100,
        "categoryName": "Resource Type",
        "categoryDescription": "The general kind of resource",
        "categoryIsFilter": true,
        "features": [
          {
            "featureId": 136,
            "name": "CPU Compute",
            "description": "General compute use",
            "isFilter": null
          },
          {
            "featureId": 133,
            "name": "Cloud",
            "description": "Cloud Resource",
            "isFilter": null
          }
        ]
      }
    ]
  },
  {
    "resourceId": 530481,
    "resourceName": "PSC Bridges-2 Regular Memory (PSC Bridges-2 RM)",
    "displayResourceName": "PSC Bridges-2 Regular Memory (PSC Bridges-2 RM)",
    "resourceType": "Compute",
    "description": "1 SU = 1 core hour",
    "units": "Core-hours",
    "organization": "Pittsburgh Supercomputing Center",
    "resourceRepositoryKey": 145121,
    "productionBeginDate": "2021-02-11",
    "productionEndDate": "2025-09-30",
    "resourceDescription": "Bridges-2 combines high-performance computing (HPC), high performance artificial intelligence (HPAI), and large-scale data management to support simulation and modeling, data analytics, community data, and complex workflows.\r\n\r\nBridges-2 Regular Memory (RM) nodes provide extremely powerful general-purpose computing, machine learning and data analytics, AI inferencing, and pre- and post-processing. Each Bridges RM node consists of two AMD EPYC “Rome” 7742 64-core CPUs, 256-512GB of RAM, and 3.84TB NVMe SSD. 488 Bridges-2 RM nodes have 256GB RAM, and 16 have 512GB RAM for more memory-intensive applications (see also Bridges-2 Extreme Memory nodes, each of which has 4TB of RAM). Bridges-2 RM nodes are connected to other Bridges-2 compute nodes and its Ocean parallel filesystem and archive by HDR-200 InfiniBand.",
    "recommendedUse": "Bridges-2 Regular Memory (RM) nodes provide extremely powerful general-purpose computing, machine learning and data analytics, AI inferencing, and pre- and post-processing. Their x86 CPUs support an extremely broad range of applications, and jobs can request anywhere from 1 core to all 64,512 cores of the Bridges-2 RM resource.",
    "conversionFactor": "8.0145",
    "userGuideUrl": "http://www.psc.edu/resources/bridges-2/user-guide",
    "attributeSets": [],
    "featureCategories": [
      {
        "categoryId": 103,
        "categoryName": "Specialized Support",
        "categoryDescription": "Special services offered by the resource",
        "categoryIsFilter": true,
        "features": [
          {
            "featureId": 167,
            "name": "ACCESS OnDemand",
            "description": "An intuitive, innovative, and interactive interface to remote computing resources",
            "isFilter": null
          },
          {
            "featureId": 166,
            "name": "ACCESS Pegasus",
            "description": "Run Jobs and Workflows on ACCESS Resources from a Single Entry Point",
            "isFilter": null
          },
          {
            "featureId": 143,
            "name": "Advance reservations",
            "description": "has advance reservations",
            "isFilter": null
          },
          {
            "featureId": 146,
            "name": "Science Gateway support",
            "description": "Support for Science Gateways",
            "isFilter": null
          }
        ]
      },
      {
        "categoryId": 101,
        "categoryName": "Allocations",
        "categoryDescription": "Whether resource is allocated by ACCESS or not",
        "categoryIsFilter": true,
        "features": [
          {
            "featureId": 139,
            "name": "ACCESS Allocated",
            "description": "Resource is allocated by ACCESS",
            "isFilter": null
          }
        ]
      },
      {
        "categoryId": 100,
        "categoryName": "Resource Type",
        "categoryDescription": "The general kind of resource",
        "categoryIsFilter": true,
        "features": [
          {
            "featureId": 136,
            "name": "CPU Compute",
            "description": "General compute use",
            "isFilter": null
          }
        ]
      },
      {
        "categoryId": 133,
        "categoryName": "Resource Category",
        "categoryDescription": "NAIRR Pilot Resource Category",
        "categoryIsFilter": true,
        "features": [
          {
            "featureId": 232,
            "name": "Federal agency systems",
            "description": "Agency supercomputers and advanced architecture systems",
            "isFilter": null
          }
        ]
      }
    ]
  },
  {
    "resourceId": 531034,
    "resourceName": "TAMU FASTER",
    "displayResourceName": "TAMU FASTER",
    "resourceType": "Compute",
    "description": "1 SU = 1 core hour",
    "units": "SUs",
    "organization": "Texas A\u0026M University",
    "resourceRepositoryKey": 145280,
    "productionBeginDate": "2022-03-31",
    "productionEndDate": "2025-09-30",
    "resourceDescription": "FASTER (Fostering Accelerated Scientific Transformations, Education and Research) is funded by the NSF MRI program (Award #2019129) and provides a composable high-performance data-analysis and computing instrument.  The FASTER system has 180 compute nodes with 2 Intel 32-core Ice Lake processors and 256 GB RAM, and includes 240 NVIDIA GPUs (40 A100 and 200 T4 GPUs). Using LIQID’s composable technology, all 180 compute nodes have access to the pool of available GPUs, dramatically improving workflow scalability. FASTER will have HDR InfiniBand interconnection and access/share a 5PB usable high-performance storage system running Lustre filesystem. thirty percent of FASTER’s computing resources will be allocated to researchers nationwide through XSEDE’s XRAC process.",
    "recommendedUse": "Workflows that can utilize multiple GPUs.",
    "conversionFactor": "1.0",
    "userGuideUrl": null,
    "attributeSets": [],
    "featureCategories": [
      {
        "categoryId": 100,
        "categoryName": "Resource Type",
        "categoryDescription": "The general kind of resource",
        "categoryIsFilter": true,
        "features": [
          {
            "featureId": 135,
            "name": "Innovative / Novel Compute",
            "description": "Unique, innovative or non-traditional compute resource",
            "isFilter": null
          },
          {
            "featureId": 134,
            "name": "GPU Compute",
            "description": "GPU use is the main purpose of this resource",
            "isFilter": null
          },
          {
            "featureId": 136,
            "name": "CPU Compute",
            "description": "General compute use",
            "isFilter": null
          }
        ]
      },
      {
        "categoryId": 101,
        "categoryName": "Allocations",
        "categoryDescription": "Whether resource is allocated by ACCESS or not",
        "categoryIsFilter": true,
        "features": [
          {
            "featureId": 139,
            "name": "ACCESS Allocated",
            "description": "Resource is allocated by ACCESS",
            "isFilter": null
          }
        ]
      },
      {
        "categoryId": 102,
        "categoryName": "Specialized Hardware",
        "categoryDescription": "Key compute hardware components in this resource",
        "categoryIsFilter": true,
        "features": [
          {
            "featureId": 141,
            "name": "Composable hardware fabric",
            "description": "Composable hardware fabric",
            "isFilter": null
          },
          {
            "featureId": 142,
            "name": "Large Memory Nodes",
            "description": "Specialized analysis nodes with large memory",
            "isFilter": null
          }
        ]
      },
      {
        "categoryId": 103,
        "categoryName": "Specialized Support",
        "categoryDescription": "Special services offered by the resource",
        "categoryIsFilter": true,
        "features": [
          {
            "featureId": 167,
            "name": "ACCESS OnDemand",
            "description": "An intuitive, innovative, and interactive interface to remote computing resources",
            "isFilter": null
          }
        ]
      }
    ]
  },
  {
    "resourceId": 530806,
    "resourceName": "Indiana Jetstream2 GPU",
    "displayResourceName": "Indiana Jetstream2 GPU",
    "resourceType": "Compute",
    "description": "Jetstream2 GPU VMs are charged 4x the number of vCPUs per hour because of the associated GPU capability. For example, the g3.large VM costs 64 SUs per hour. VM sizes and cost per hour are available at \u003ca href=\"https://docs.jetstream-cloud.org/general/vmsizes/\"\u003ethe Jetstream2 website\u003c/a\u003e.",
    "units": "SUs",
    "organization": "Indiana University",
    "resourceRepositoryKey": 145447,
    "productionBeginDate": "2021-10-01",
    "productionEndDate": "2025-09-30",
    "resourceDescription": "Jetstream2 GPU is a hybrid-cloud platform that provides flexible, on-demand, programmable cyberinfrastructure tools ranging from interactive virtual machine services to a variety of infrastructure and orchestration services for research and education. This particular portion of the resource is allocated separately from the primary resource and contains 360 NVIDIA A100 GPUs -- 4 GPUs per node, 128 AMD Milan cores, and 512gb RAM connected by 100gbps ethernet to the spine.",
    "recommendedUse": "For the researcher needing GPUs for virtual machine services on demand as well as for software creators and researchers needing to create their own customized virtual machine environments. Additional use cases are for research-supporting infrastructure services that need to be \"always on\" as well as science gateway services and for education support, providing virtual machines for students. \r\n\r\nThe A100 GPUs on Jetstream2-GPU are well-suited for machine learning/deep learning projects and other codes optimized for GPU usage. They also may be utilized for some graphical/desktop applications with some effort.",
    "conversionFactor": "8.0145",
    "userGuideUrl": "https://docs.jetstream-cloud.org/",
    "attributeSets": [],
    "featureCategories": [
      {
        "categoryId": 103,
        "categoryName": "Specialized Support",
        "categoryDescription": "Special services offered by the resource",
        "categoryIsFilter": true,
        "features": [
          {
            "featureId": 166,
            "name": "ACCESS Pegasus",
            "description": "Run Jobs and Workflows on ACCESS Resources from a Single Entry Point",
            "isFilter": null
          },
          {
            "featureId": 146,
            "name": "Science Gateway support",
            "description": "Support for Science Gateways",
            "isFilter": null
          }
        ]
      },
      {
        "categoryId": 101,
        "categoryName": "Allocations",
        "categoryDescription": "Whether resource is allocated by ACCESS or not",
        "categoryIsFilter": true,
        "features": [
          {
            "featureId": 139,
            "name": "ACCESS Allocated",
            "description": "Resource is allocated by ACCESS",
            "isFilter": null
          }
        ]
      },
      {
        "categoryId": 100,
        "categoryName": "Resource Type",
        "categoryDescription": "The general kind of resource",
        "categoryIsFilter": true,
        "features": [
          {
            "featureId": 134,
            "name": "GPU Compute",
            "description": "GPU use is the main purpose of this resource",
            "isFilter": null
          },
          {
            "featureId": 133,
            "name": "Cloud",
            "description": "Cloud Resource",
            "isFilter": null
          }
        ]
      },
      {
        "categoryId": 133,
        "categoryName": "Resource Category",
        "categoryDescription": "NAIRR Pilot Resource Category",
        "categoryIsFilter": true,
        "features": [
          {
            "featureId": 232,
            "name": "Federal agency systems",
            "description": "Agency supercomputers and advanced architecture systems",
            "isFilter": null
          }
        ]
      }
    ]
  },
  {
    "resourceId": 530484,
    "resourceName": "PSC Bridges-2 GPU (PSC Bridges-2 GPU)",
    "displayResourceName": "PSC Bridges-2 GPU (PSC Bridges-2 GPU)",
    "resourceType": "Compute",
    "description": "1 SU = 1 GPU hour",
    "units": "GPU Hours",
    "organization": "Pittsburgh Supercomputing Center",
    "resourceRepositoryKey": 145124,
    "productionBeginDate": "2021-02-11",
    "productionEndDate": "2025-09-30",
    "resourceDescription": "Bridges-2 combines high-performance computing (HPC), high performance artificial intelligence (HPAI), and large-scale data management to support simulation and modeling, data analytics, community data, and complex workflows.\r\n\r\nBridges-2 Accelerated GPU (GPU) nodes are optimized for scalable artificial intelligence (AI; deep learning). They are also available for accelerated simulation and modeling applications. Bridges-2 GPU nodes each contain 8 NVIDIA Tesla V100-32GB SXM2 GPUs, providing 40,960 CUDA cores and 5,120 tensor cores. In addition, each node holds 2 Intel Xeon Gold 6248 CPUs; 512GB of DDR4-2933 RAM; and 7.68TB NVMe SSD. They are connected to Bridges-2's other compute nodes and its Ocean parallel filesystem and archive by two HDR-200 InfiniBand links, providing 400Gbps of bandwidth to enhance scalability of deep learning training.",
    "recommendedUse": "Bridges-2 GPU nodes are optimized for scalable artificial intelligence (AI) including deep learning training, deep reinforcement learning, and generative techniques - as well as for accelerated simulation and modeling.",
    "conversionFactor": "431.55",
    "userGuideUrl": "http://www.psc.edu/resources/bridges-2/user-guide",
    "attributeSets": [],
    "featureCategories": [
      {
        "categoryId": 101,
        "categoryName": "Allocations",
        "categoryDescription": "Whether resource is allocated by ACCESS or not",
        "categoryIsFilter": true,
        "features": [
          {
            "featureId": 139,
            "name": "ACCESS Allocated",
            "description": "Resource is allocated by ACCESS",
            "isFilter": null
          }
        ]
      },
      {
        "categoryId": 103,
        "categoryName": "Specialized Support",
        "categoryDescription": "Special services offered by the resource",
        "categoryIsFilter": true,
        "features": [
          {
            "featureId": 166,
            "name": "ACCESS Pegasus",
            "description": "Run Jobs and Workflows on ACCESS Resources from a Single Entry Point",
            "isFilter": null
          },
          {
            "featureId": 167,
            "name": "ACCESS OnDemand",
            "description": "An intuitive, innovative, and interactive interface to remote computing resources",
            "isFilter": null
          },
          {
            "featureId": 143,
            "name": "Advance reservations",
            "description": "has advance reservations",
            "isFilter": null
          },
          {
            "featureId": 146,
            "name": "Science Gateway support",
            "description": "Support for Science Gateways",
            "isFilter": null
          }
        ]
      },
      {
        "categoryId": 100,
        "categoryName": "Resource Type",
        "categoryDescription": "The general kind of resource",
        "categoryIsFilter": true,
        "features": [
          {
            "featureId": 134,
            "name": "GPU Compute",
            "description": "GPU use is the main purpose of this resource",
            "isFilter": null
          }
        ]
      },
      {
        "categoryId": 133,
        "categoryName": "Resource Category",
        "categoryDescription": "NAIRR Pilot Resource Category",
        "categoryIsFilter": true,
        "features": [
          {
            "featureId": 232,
            "name": "Federal agency systems",
            "description": "Agency supercomputers and advanced architecture systems",
            "isFilter": null
          }
        ]
      }
    ]
  },
  {
    "resourceId": 530209,
    "resourceName": "Science Gateways Center of Excellence (SGX3)",
    "displayResourceName": "Science Gateways Center of Excellence (SGX3)",
    "resourceType": "Program",
    "description": "",
    "units": "Yes / No",
    "organization": "Science Gateways Community Institute",
    "resourceRepositoryKey": 144601,
    "productionBeginDate": "2016-08-01",
    "productionEndDate": "2027-08-31",
    "resourceDescription": "The Science Gateways Community Institute (SGCI) and the follow-on SGX3 NSF Center of Excellence projects support science gateways through both complementary and paid services. Complementary services include assistance with proposal development, a technical consultancy to help science gateway providers define their system architectures and choose technologies, a usability/user experience consultancy to help design effective user interfaces, and sustainability training programs to help science gateway providers with long term operations planning.\r\nFor more about science gateways and additional available services, please see https://sciencegateways.org/our-services/consulting-services.  An SGCI team member will contact allocated projects to help review and select specific services.",
    "recommendedUse": "SGCI services can be used to develop entirely new gateways or improve existing gateways. The gateways do not need to make use of ACCESS compute resources (though they can). They could be gateways to data collections, instruments, sensor streams, citizen science engagement, etc.",
    "conversionFactor": "8.0145",
    "userGuideUrl": "http://www.sciencegateways.org/",
    "attributeSets": [
      {
        "attributeSetId": 530701,
        "attributeSetTypeId": 500008,
        "attributeSetRelationTypeId": 500007,
        "attributeSetName": "Science Gateway Name",
        "relativeOrder": 1,
        "isActive": true,
        "attributes": [
          {
            "resourceAttributeId": 533321,
            "attributeName": "Please enter your answer below:",
            "description": null,
            "isRequired": true,
            "relativeOrder": 1
          }
        ]
      },
      {
        "attributeSetId": 530702,
        "attributeSetTypeId": 500008,
        "attributeSetRelationTypeId": 500007,
        "attributeSetName": "Science Gateway URL (if applicable)",
        "relativeOrder": 2,
        "isActive": true,
        "attributes": [
          {
            "resourceAttributeId": 533322,
            "attributeName": "Please enter your answer below:",
            "description": null,
            "isRequired": true,
            "relativeOrder": 1
          }
        ]
      },
      {
        "attributeSetId": 530703,
        "attributeSetTypeId": 500008,
        "attributeSetRelationTypeId": 500007,
        "attributeSetName": "Other Team Members who will work on the project (optional; please include contact info for them)",
        "relativeOrder": 3,
        "isActive": true,
        "attributes": [
          {
            "resourceAttributeId": 533323,
            "attributeName": "Please enter your answer below:",
            "description": null,
            "isRequired": true,
            "relativeOrder": 1
          }
        ]
      },
      {
        "attributeSetId": 530704,
        "attributeSetTypeId": 500008,
        "attributeSetRelationTypeId": 500007,
        "attributeSetName": "Project Description: Please provide a short description summarizing the science gateway project, its goals and functions, any technologies already in use, and the community that it serves or will serve.",
        "relativeOrder": 4,
        "isActive": true,
        "attributes": [
          {
            "resourceAttributeId": 533324,
            "attributeName": "Please enter your answer below:",
            "description": null,
            "isRequired": true,
            "relativeOrder": 1
          }
        ]
      },
      {
        "attributeSetId": 530705,
        "attributeSetTypeId": 500008,
        "attributeSetRelationTypeId": 500007,
        "attributeSetName": "SGCI Services: Please provide a short description about what you would like to achieve by using the services of the Institute.",
        "relativeOrder": 5,
        "isActive": true,
        "attributes": [
          {
            "resourceAttributeId": 533325,
            "attributeName": "Please enter your answer below:",
            "description": null,
            "isRequired": true,
            "relativeOrder": 1
          }
        ]
      }
    ],
    "featureCategories": [
      {
        "categoryId": 100,
        "categoryName": "Resource Type",
        "categoryDescription": "The general kind of resource",
        "categoryIsFilter": true,
        "features": [
          {
            "featureId": 137,
            "name": "Service / Other",
            "description": "An allocated resource that's not compute or storage",
            "isFilter": null
          }
        ]
      },
      {
        "categoryId": 101,
        "categoryName": "Allocations",
        "categoryDescription": "Whether resource is allocated by ACCESS or not",
        "categoryIsFilter": true,
        "features": [
          {
            "featureId": 139,
            "name": "ACCESS Allocated",
            "description": "Resource is allocated by ACCESS",
            "isFilter": null
          }
        ]
      },
      {
        "categoryId": 103,
        "categoryName": "Specialized Support",
        "categoryDescription": "Special services offered by the resource",
        "categoryIsFilter": true,
        "features": [
          {
            "featureId": 146,
            "name": "Science Gateway support",
            "description": "Support for Science Gateways",
            "isFilter": null
          }
        ]
      }
    ]
  },
  {
    "resourceId": 531364,
    "resourceName": "MATCHPlus",
    "displayResourceName": "MATCHPlus",
    "resourceType": "Program",
    "description": "The ACCESS MATCHPlus short-term support partnership provides consulting support from an experienced mentor paired with a student-facilitator to help you address an immediate research need. These needs may include improvements like expanding your code functionality, transitioning from lab computers to HPC, or introducing new technologies into your workflow. For more information visit the ACCESS Support portal: https://support.access-ci.org/matchplus",
    "units": "Yes / No",
    "organization": "ACCESS Support",
    "resourceRepositoryKey": 146003,
    "productionBeginDate": "2022-10-24",
    "productionEndDate": null,
    "resourceDescription": null,
    "recommendedUse": "MATCHPlus is an opportunity for researchers to get help with improvements like expanding your code functionality, transitioning from lab computers to HPC, or introducing new technologies into your workflow.",
    "conversionFactor": null,
    "userGuideUrl": "https://support.access-ci.org/matchplus",
    "attributeSets": [
      {
        "attributeSetId": 535543,
        "attributeSetTypeId": 500007,
        "attributeSetRelationTypeId": 500007,
        "attributeSetName": "Please let us know why you are requesting a MatchPlus project (e.g., graduate student, first time running on a supercomputer, a small or resource-constrained institution).",
        "relativeOrder": 1,
        "isActive": true,
        "attributes": [
          {
            "resourceAttributeId": 536097,
            "attributeName": "Please enter your answer",
            "description": null,
            "isRequired": true,
            "relativeOrder": 1
          }
        ]
      }
    ],
    "featureCategories": [
      {
        "categoryId": 100,
        "categoryName": "Resource Type",
        "categoryDescription": "The general kind of resource",
        "categoryIsFilter": true,
        "features": [
          {
            "featureId": 137,
            "name": "Service / Other",
            "description": "An allocated resource that's not compute or storage",
            "isFilter": null
          }
        ]
      },
      {
        "categoryId": 101,
        "categoryName": "Allocations",
        "categoryDescription": "Whether resource is allocated by ACCESS or not",
        "categoryIsFilter": true,
        "features": [
          {
            "featureId": 139,
            "name": "ACCESS Allocated",
            "description": "Resource is allocated by ACCESS",
            "isFilter": null
          }
        ]
      }
    ]
  },
  {
    "resourceId": 530483,
    "resourceName": "PSC Bridges-2 Extreme Memory (PSC Bridges-2 EM)",
    "displayResourceName": "PSC Bridges-2 Extreme Memory (PSC Bridges-2 EM)",
    "resourceType": "Compute",
    "description": "1 SU = 1 core hour",
    "units": "Core-hours",
    "organization": "Pittsburgh Supercomputing Center",
    "resourceRepositoryKey": 145123,
    "productionBeginDate": "2021-02-11",
    "productionEndDate": "2025-09-30",
    "resourceDescription": "Bridges-2 combines high-performance computing (HPC), high performance artificial intelligence (HPAI), and large-scale data management to support simulation and modeling, data analytics, community data, and complex workflows.\r\n\r\nBridges-2 Extreme Memory (EM) nodes enable memory-intensive  genome sequence assembly, graph analytics, in-memory databases, statistics, and other applications that need a large amount of memory and for which distributed-memory implementations are not available. Bridges-2 Extreme Memory (EM) nodes each consist of 4 Intel Xeon Platinum 8260M “Cascade Lake” CPUs, 4TB of DDR4-2933 RAM, 7.68TB NVMe SSD. They are connected to Bridges-2's other compute nodes and its Ocean parallel filesystem and archive by two HDR-200 InfiniBand links, providing 400Gbps of bandwidth to read or write data from each EM node.",
    "recommendedUse": "Bridges-2 Extreme Memory (EM) nodes enable memory-intensive  genome sequence assembly, graph analytics, statistics, in-memory databases, and other applications that need a large amount of memory and for which distributed-memory implementations are not available. This includes memory-intensive applications implemented in languages such as Java, R, and Python. Their x86 CPUs support an extremely broad range of applications, and approximately 42GB of RAM per core provides valuable support for applications where memory capacity is the primary requirement.",
    "conversionFactor": "71.558",
    "userGuideUrl": "http://www.psc.edu/resources/bridges-2/user-guide",
    "attributeSets": [],
    "featureCategories": [
      {
        "categoryId": 103,
        "categoryName": "Specialized Support",
        "categoryDescription": "Special services offered by the resource",
        "categoryIsFilter": true,
        "features": [
          {
            "featureId": 166,
            "name": "ACCESS Pegasus",
            "description": "Run Jobs and Workflows on ACCESS Resources from a Single Entry Point",
            "isFilter": null
          },
          {
            "featureId": 143,
            "name": "Advance reservations",
            "description": "has advance reservations",
            "isFilter": null
          },
          {
            "featureId": 146,
            "name": "Science Gateway support",
            "description": "Support for Science Gateways",
            "isFilter": null
          },
          {
            "featureId": 167,
            "name": "ACCESS OnDemand",
            "description": "An intuitive, innovative, and interactive interface to remote computing resources",
            "isFilter": null
          }
        ]
      },
      {
        "categoryId": 101,
        "categoryName": "Allocations",
        "categoryDescription": "Whether resource is allocated by ACCESS or not",
        "categoryIsFilter": true,
        "features": [
          {
            "featureId": 139,
            "name": "ACCESS Allocated",
            "description": "Resource is allocated by ACCESS",
            "isFilter": null
          }
        ]
      },
      {
        "categoryId": 102,
        "categoryName": "Specialized Hardware",
        "categoryDescription": "Key compute hardware components in this resource",
        "categoryIsFilter": true,
        "features": [
          {
            "featureId": 142,
            "name": "Large Memory Nodes",
            "description": "Specialized analysis nodes with large memory",
            "isFilter": null
          }
        ]
      },
      {
        "categoryId": 100,
        "categoryName": "Resource Type",
        "categoryDescription": "The general kind of resource",
        "categoryIsFilter": true,
        "features": [
          {
            "featureId": 136,
            "name": "CPU Compute",
            "description": "General compute use",
            "isFilter": null
          }
        ]
      },
      {
        "categoryId": 133,
        "categoryName": "Resource Category",
        "categoryDescription": "NAIRR Pilot Resource Category",
        "categoryIsFilter": true,
        "features": [
          {
            "featureId": 232,
            "name": "Federal agency systems",
            "description": "Agency supercomputers and advanced architecture systems",
            "isFilter": null
          }
        ]
      }
    ]
  },
  {
    "resourceId": 530478,
    "resourceName": "SDSC Expanse CPU",
    "displayResourceName": "SDSC Expanse CPU",
    "resourceType": "Compute",
    "description": "SDSC Expanse Dell/AMD Rome cluster with HDR InfiniBand designed to provide cyberinfrastructure for the long tail of science, covering a diverse application base with complex workflows.",
    "units": "Core-hours",
    "organization": "San Diego Supercomputer Center",
    "resourceRepositoryKey": 145117,
    "productionBeginDate": "2020-10-01",
    "productionEndDate": "2025-10-01",
    "resourceDescription": "Expanse is a Dell integrated compute cluster, with AMD Rome processors, 128 cores per node, interconnected with Mellanox HDR InfiniBand in a hybrid fat-tree topology. The compute node section of Expanse has a peak performance of 3.373 PF. Full bisection bandwidth is available at rack level (56 compute nodes) with HDR100 connectivity to each node. HDR200 switches are used at the rack level and 3:1 oversubscription cross-rack. Compute nodes feature 1TB of NVMe storage and 256GB of DRAM per node. The system also features 12PB of Lustre based performance storage (140GB/s aggregate), and 7PB of Ceph based object storage.",
    "recommendedUse": "Expanse is designed to provide cyberinfrastructure for the long tail of science, covering a diverse application base with complex workflows. It features a rich base of preinstalled applications including commercial software like Gaussian, Abaqus, QChem, MATLAB, and IDL. The system is geared towards supporting capacity computing, optimized for quick turnaround on small/modest scale jobs. The local NVMes on each compute node are beneficial to applications that exhibit random access data patterns or require fast access to significant amounts of compute node local scratch space. Expanse supports containers and composable systems computing with dynamic capabilities enabled using tools such as Kubernetes and workflow software for e.g., continuum/edge computing applications.",
    "conversionFactor": "8.0145",
    "userGuideUrl": "https://www.sdsc.edu/support/user_guides/expanse.html",
    "attributeSets": [
      {
        "attributeSetId": 536171,
        "attributeSetTypeId": 500006,
        "attributeSetRelationTypeId": 500004,
        "attributeSetName": "Do your jobs require less than 4096 cores? Expanse CPU jobs are limited in size to 4096 cores (32 nodes). Up to 4096 single-core jobs are permitted simultaneously.",
        "relativeOrder": 7,
        "isActive": true,
        "attributes": [
          {
            "resourceAttributeId": 536462,
            "attributeName": "Yes",
            "description": null,
            "isRequired": true,
            "relativeOrder": 1
          },
          {
            "resourceAttributeId": 536463,
            "attributeName": "No",
            "description": null,
            "isRequired": true,
            "relativeOrder": 1
          }
        ]
      }
    ],
    "featureCategories": [
      {
        "categoryId": 103,
        "categoryName": "Specialized Support",
        "categoryDescription": "Special services offered by the resource",
        "categoryIsFilter": true,
        "features": [
          {
            "featureId": 166,
            "name": "ACCESS Pegasus",
            "description": "Run Jobs and Workflows on ACCESS Resources from a Single Entry Point",
            "isFilter": null
          },
          {
            "featureId": 143,
            "name": "Advance reservations",
            "description": "has advance reservations",
            "isFilter": null
          },
          {
            "featureId": 167,
            "name": "ACCESS OnDemand",
            "description": "An intuitive, innovative, and interactive interface to remote computing resources",
            "isFilter": null
          },
          {
            "featureId": 146,
            "name": "Science Gateway support",
            "description": "Support for Science Gateways",
            "isFilter": null
          }
        ]
      },
      {
        "categoryId": 101,
        "categoryName": "Allocations",
        "categoryDescription": "Whether resource is allocated by ACCESS or not",
        "categoryIsFilter": true,
        "features": [
          {
            "featureId": 139,
            "name": "ACCESS Allocated",
            "description": "Resource is allocated by ACCESS",
            "isFilter": null
          }
        ]
      },
      {
        "categoryId": 102,
        "categoryName": "Specialized Hardware",
        "categoryDescription": "Key compute hardware components in this resource",
        "categoryIsFilter": true,
        "features": [
          {
            "featureId": 142,
            "name": "Large Memory Nodes",
            "description": "Specialized analysis nodes with large memory",
            "isFilter": null
          }
        ]
      },
      {
        "categoryId": 100,
        "categoryName": "Resource Type",
        "categoryDescription": "The general kind of resource",
        "categoryIsFilter": true,
        "features": [
          {
            "featureId": 136,
            "name": "CPU Compute",
            "description": "General compute use",
            "isFilter": null
          }
        ]
      },
      {
        "categoryId": 133,
        "categoryName": "Resource Category",
        "categoryDescription": "NAIRR Pilot Resource Category",
        "categoryIsFilter": true,
        "features": [
          {
            "featureId": 232,
            "name": "Federal agency systems",
            "description": "Agency supercomputers and advanced architecture systems",
            "isFilter": null
          }
        ]
      }
    ]
  },
  {
    "resourceId": 530480,
    "resourceName": "SDSC Expanse GPU",
    "displayResourceName": "SDSC Expanse GPU",
    "resourceType": "Compute",
    "description": "SDSC Expanse Dell/NVIDIA V100 cluster with NVLINK and HDR InfiniBand recommend to be used for accelerating codes optimized to take advantage of GPUs.",
    "units": "GPU Hours",
    "organization": "San Diego Supercomputer Center",
    "resourceRepositoryKey": 145118,
    "productionBeginDate": "2020-10-01",
    "productionEndDate": "2025-10-01",
    "resourceDescription": "Expanse is a Dell integrated compute cluster, with AMD Rome processors, NVIDIA V100 GPUs, interconnected with Mellanox HDR InfiniBand in a hybrid fat-tree topology. The GPU component of Expanse features 52 GPU nodes, each containing four NVIDIA V100s (32 GB SMX2), connected via NVLINK, and dual 20-core Intel Xeon 6248 CPUs. They feature 1.6TB of NVMe storage and 256GB of DRAM per node. There is HDR100 connectivity to each node. The system also features 12PB of Lustre based performance storage (140GB/s aggregate), and 7PB of Ceph based object storage.",
    "recommendedUse": "GPUs are a specialized resource that performs well for certain classes of algorithms and applications. Recommend to be used for accelerating simulation codes optimized to take advantage of GPUs (using CUDA, OpenACC). There is a large and growing base of community codes that have been optimized for GPUs including those in molecular dynamics, and machine learning. GPU-enabled applications on Expanse will include: AMBER, Gromacs, BEAST, OpenMM, NAMD, TensorFlow, and PyTorch.",
    "conversionFactor": "431.55",
    "userGuideUrl": "https://www.sdsc.edu/support/user_guides/expanse.html",
    "attributeSets": [
      {
        "attributeSetId": 536170,
        "attributeSetTypeId": 500006,
        "attributeSetRelationTypeId": 500004,
        "attributeSetName": "Do your jobs require less than 16 GPUs in a single job? Expanse GPU jobs are limited in size to 16 GPUs (4 nodes). Up to 24 single GPU-node jobs are permitted simultaneously.",
        "relativeOrder": 7,
        "isActive": true,
        "attributes": [
          {
            "resourceAttributeId": 536460,
            "attributeName": "Yes",
            "description": null,
            "isRequired": true,
            "relativeOrder": 1
          },
          {
            "resourceAttributeId": 536461,
            "attributeName": "No",
            "description": null,
            "isRequired": true,
            "relativeOrder": 1
          }
        ]
      }
    ],
    "featureCategories": [
      {
        "categoryId": 101,
        "categoryName": "Allocations",
        "categoryDescription": "Whether resource is allocated by ACCESS or not",
        "categoryIsFilter": true,
        "features": [
          {
            "featureId": 139,
            "name": "ACCESS Allocated",
            "description": "Resource is allocated by ACCESS",
            "isFilter": null
          }
        ]
      },
      {
        "categoryId": 103,
        "categoryName": "Specialized Support",
        "categoryDescription": "Special services offered by the resource",
        "categoryIsFilter": true,
        "features": [
          {
            "featureId": 166,
            "name": "ACCESS Pegasus",
            "description": "Run Jobs and Workflows on ACCESS Resources from a Single Entry Point",
            "isFilter": null
          },
          {
            "featureId": 143,
            "name": "Advance reservations",
            "description": "has advance reservations",
            "isFilter": null
          },
          {
            "featureId": 146,
            "name": "Science Gateway support",
            "description": "Support for Science Gateways",
            "isFilter": null
          },
          {
            "featureId": 167,
            "name": "ACCESS OnDemand",
            "description": "An intuitive, innovative, and interactive interface to remote computing resources",
            "isFilter": null
          }
        ]
      },
      {
        "categoryId": 100,
        "categoryName": "Resource Type",
        "categoryDescription": "The general kind of resource",
        "categoryIsFilter": true,
        "features": [
          {
            "featureId": 134,
            "name": "GPU Compute",
            "description": "GPU use is the main purpose of this resource",
            "isFilter": null
          }
        ]
      },
      {
        "categoryId": 133,
        "categoryName": "Resource Category",
        "categoryDescription": "NAIRR Pilot Resource Category",
        "categoryIsFilter": true,
        "features": [
          {
            "featureId": 232,
            "name": "Federal agency systems",
            "description": "Agency supercomputers and advanced architecture systems",
            "isFilter": null
          }
        ]
      }
    ]
  },
  {
    "resourceId": 531529,
    "resourceName": "TAMU ACES",
    "displayResourceName": "TAMU ACES",
    "resourceType": "Compute",
    "description": "",
    "units": "SUs",
    "organization": "Texas A\u0026M University",
    "resourceRepositoryKey": 146168,
    "productionBeginDate": "2023-04-14",
    "productionEndDate": "2026-09-30",
    "resourceDescription": "ACES is a Dell cluster with a rich accelerator testbed consisting of Intel Max GPUs (Graphics Processing Units), Intel FPGAs (Field Programmable Gate Arrays), NVIDIA H100 and A30 GPUs, NEC Vector Engines, NextSilicon co-processors, Graphcore IPUs (Intelligence Processing Units). The ACES cluster consists of compute nodes using a mix of the following processors:\r\n\r\nIntel Xeon 8468 Sapphire Rapids processors\r\nIntel Xeon Ice Lake 8352Y processors\r\nIntel Xeon Cascade Lake 8268 processors\r\nAMD Epyc Rome 7742 processors\r\n\r\nThe compute nodes are interconnected with NVIDIA NDR200 connections for MPI and access to the Lustre storage. The Intel Optane SSDs and all accelerators (except the Graphcore IPUs and NEC Vector Engines) are accessed using Liqid's composable infrustructre via PCIe (Peripheral Component Interconnect express) Gen4 and Gen5 fabrics.",
    "recommendedUse": "Machine Learning (ML), Artificial Intelligence (AI), and workflows that can utilize novel accelerators and/or multiple GPUs.",
    "conversionFactor": "1.0",
    "userGuideUrl": "https://hprc.tamu.edu/kb/Quick-Start/ACES",
    "attributeSets": [],
    "featureCategories": [
      {
        "categoryId": 101,
        "categoryName": "Allocations",
        "categoryDescription": "Whether resource is allocated by ACCESS or not",
        "categoryIsFilter": true,
        "features": [
          {
            "featureId": 139,
            "name": "ACCESS Allocated",
            "description": "Resource is allocated by ACCESS",
            "isFilter": null
          }
        ]
      },
      {
        "categoryId": 100,
        "categoryName": "Resource Type",
        "categoryDescription": "The general kind of resource",
        "categoryIsFilter": true,
        "features": [
          {
            "featureId": 134,
            "name": "GPU Compute",
            "description": "GPU use is the main purpose of this resource",
            "isFilter": null
          },
          {
            "featureId": 135,
            "name": "Innovative / Novel Compute",
            "description": "Unique, innovative or non-traditional compute resource",
            "isFilter": null
          },
          {
            "featureId": 136,
            "name": "CPU Compute",
            "description": "General compute use",
            "isFilter": null
          }
        ]
      },
      {
        "categoryId": 102,
        "categoryName": "Specialized Hardware",
        "categoryDescription": "Key compute hardware components in this resource",
        "categoryIsFilter": true,
        "features": [
          {
            "featureId": 141,
            "name": "Composable hardware fabric",
            "description": "Composable hardware fabric",
            "isFilter": null
          }
        ]
      },
      {
        "categoryId": 103,
        "categoryName": "Specialized Support",
        "categoryDescription": "Special services offered by the resource",
        "categoryIsFilter": true,
        "features": [
          {
            "featureId": 167,
            "name": "ACCESS OnDemand",
            "description": "An intuitive, innovative, and interactive interface to remote computing resources",
            "isFilter": null
          },
          {
            "featureId": 146,
            "name": "Science Gateway support",
            "description": "Support for Science Gateways",
            "isFilter": null
          }
        ]
      },
      {
        "categoryId": 133,
        "categoryName": "Resource Category",
        "categoryDescription": "NAIRR Pilot Resource Category",
        "categoryIsFilter": true,
        "features": [
          {
            "featureId": 232,
            "name": "Federal agency systems",
            "description": "Agency supercomputers and advanced architecture systems",
            "isFilter": null
          }
        ]
      }
    ]
  },
  {
    "resourceId": 531760,
    "resourceName": "SDSC Voyager (Habana Training and Inference Processor based AI System)",
    "displayResourceName": "SDSC Voyager (Habana Training and Inference Processor based AI System)",
    "resourceType": "Compute",
    "description": "SDSC Voyager cluster with Intel Habana Gaudi training processors for deep learning applications. Voyager does not have general-purpose GPUs. The Gaudi training processors are custom designed and optimized for deep learning. Only AI/scientific applications that feature deep learning are recommended for use on this system.\r\n",
    "units": "Gaudi-hours",
    "organization": "San Diego Supercomputer Center",
    "resourceRepositoryKey": 146630,
    "productionBeginDate": "2022-05-01",
    "productionEndDate": null,
    "resourceDescription": "Voyager is a heterogeneous system designed to support complex deep learning AI workflows. The system features 42 Intel Habana Gaudi training nodes, each with 8 training processors (336 in total). Each training node has 512GB of memory and 6.4TB of node local NVMe storage. The Gaudi training processors feature specialized hardware units for AI, HBM2, and on-chip high-speed Ethernet. The on-chip ethernet ports are used in a non-blocking all-to-all network between processors on a node and the remaining ports are aggregated into 6 400G connections on each node that are plugged into a 400G Arista switch to provide scale out of network. Voyager also has two first-generation inference nodes, each with 8 inference processors (16 in total). In addition to the custom AI hardware, the system also has 36 Intel x86 processors compute nodes for general purpose computing and data processing. Voyager features 3PB of storage currently deployed as a Ceph filesystem.",
    "recommendedUse": "Voyager invites researchers in science and engineering that are dependent upon artificial intelligence and deep learning as a critical element in their experimental and/or computational work. It provides researchers the ability to work with extremely large data sets using standard AI tools, like TensorFlow and PyTorch, or develop their own deep learning models using developer tools and libraries from Habana Labs. Voyager is currently in the testbed phase and accepting researchers through a RP managed allocation process. Interested researchers are requested to contact SDSC via email at consult@sdsc.edu.",
    "conversionFactor": null,
    "userGuideUrl": "https://sdsc.edu/support/user_guides/voyager.html",
    "attributeSets": [],
    "featureCategories": [
      {
        "categoryId": 100,
        "categoryName": "Resource Type",
        "categoryDescription": "The general kind of resource",
        "categoryIsFilter": true,
        "features": [
          {
            "featureId": 135,
            "name": "Innovative / Novel Compute",
            "description": "Unique, innovative or non-traditional compute resource",
            "isFilter": null
          }
        ]
      },
      {
        "categoryId": 101,
        "categoryName": "Allocations",
        "categoryDescription": "Whether resource is allocated by ACCESS or not",
        "categoryIsFilter": true,
        "features": [
          {
            "featureId": 140,
            "name": "RP-managed process",
            "description": "Resource is not allocated by ACCESS",
            "isFilter": null
          }
        ]
      },
      {
        "categoryId": 133,
        "categoryName": "Resource Category",
        "categoryDescription": "NAIRR Pilot Resource Category",
        "categoryIsFilter": true,
        "features": [
          {
            "featureId": 232,
            "name": "Federal agency systems",
            "description": "Agency supercomputers and advanced architecture systems",
            "isFilter": null
          }
        ]
      }
    ]
  },
  {
    "resourceId": 531727,
    "resourceName": "PSC Neocortex (PSC Neocortex - Cerebras Wafer Scale Engine AI Accelerator)",
    "displayResourceName": "PSC Neocortex (PSC Neocortex - Cerebras Wafer Scale Engine AI Accelerator)",
    "resourceType": "Compute",
    "description": "The Neocortex is a testbed system incorporating the Cerebras Wafer Engine (WSE), an accelerator initially designed for optimizing AI training and inferencing. Since its introduction a few years ago, the chip has demonstrated its effectiveness in accelerating specific non-AI workflows as well. Notably, the WSE is utilized in lieu of other accelerators, such as GPUs.\r\n\r\nNeocortex projects are expected to require the powerful compute capabilities offered by the Cerebras server and the wafer scale engine chip. Neocortex is ideal for a well-defined group of foundation and large language models. Please refer to \u003ca href=\"https://portal.neocortex.psc.edu/docs/index.html\" target=\"_blank\"\u003ethe Neocortex user guide\u003c/a\u003e for more information on the models supported and the type of applications ideally suited for Neocortex.",
    "units": "CS-2 Hours",
    "organization": "Pittsburgh Supercomputing Center",
    "resourceRepositoryKey": 146597,
    "productionBeginDate": "2023-12-01",
    "productionEndDate": null,
    "resourceDescription": "Neocortex is a highly innovative advanced computing system ideal for foundation and large language models. Neocortex, which captures promising specialized innovative hardware technologies, is designed to vastly accelerate large deep learning (DL) models and high- performance computing (HPC) research in pursuit of science, discovery, and societal good. Neocortex features two Cerebras CS-2 systems, provisioned by an HPE Superdome Flex HPC server and the Bridges-2 filesystems. Each CS-2 system features a Cerebras WSE-2 (Wafer Scale Engine 2), the largest chip ever built, with 850,000 Sparse Linear Algebra Compute cores, 40 GB SRAM on-chip memory, 20 PB/s aggregate memory bandwidth and 220 Pb/s interconnect bandwidth. The HPE Superdome Flex (SDF) features 32 Intel Xeon Platinum 8280L CPUs with 28 cores (56 threads) each, 2.70-4.0 GHz, 38.5 MB cache, 24 TiB RAM, aggregate memory bandwidth of 4.5 TB/s, and 204.6 TB aggregate local storage capacity with 150 GB/s read bandwidth. The SDF can provide 1.2 Tb/s to each CS-2 system and 1.6 Tb/s from the Bridges-2 filesystems. Jobs are submitted via SLURM. The CS-2 systems can run customized TensorFlow and Pytorch containers, as well as programs written using the Cerebras SDK or the WSE Field Equation API.",
    "recommendedUse": "Neocortex, a system that captures promising innovative hardware technologies, is designed to accelerate Deep Learning (DL) and High Performance Computing (HPC) research in pursuit of science, discovery, and societal good. Currently recommended DL projects focus on foundation and large language models such as BERT, GPT-J, and Transformer, or combine supported TensorFlow or PyTorch layers. DL codes can also be developed “from scratch” using the Cerebras Software Development Toolkit (SDK). The SDK can be used to develop HPC codes, such as structured grid based PDE and ODE solvers and particle methods with regular communication. Interested researchers are encouraged to contact PSC via email at neocortex@psc.edu to address comments and questions.",
    "conversionFactor": "17631.9",
    "userGuideUrl": "https://www.cmu.edu/psc/aibd/neocortex/",
    "attributeSets": [
      {
        "attributeSetId": 536038,
        "attributeSetTypeId": 500007,
        "attributeSetRelationTypeId": 500006,
        "attributeSetName": "Describe your research team (*). Please, describe your research team and make sure to include expected roles in the proposed research, current affiliation, academic status, and relevant experience. Links to professional pages or attached documents that describe relevant experience and qualifications are welcomed.",
        "relativeOrder": 1,
        "isActive": true,
        "attributes": [
          {
            "resourceAttributeId": 536394,
            "attributeName": "Please enter your answer",
            "description": null,
            "isRequired": false,
            "relativeOrder": 1
          }
        ]
      },
      {
        "attributeSetId": 536039,
        "attributeSetTypeId": 500007,
        "attributeSetRelationTypeId": 500006,
        "attributeSetName": "Describe your data and the total storage requirements in GB (*). Please, describe your data format and size in GB. Make sure to request as many GB of PSC Bridges-2 Ocean storage.",
        "relativeOrder": 2,
        "isActive": true,
        "attributes": [
          {
            "resourceAttributeId": 536395,
            "attributeName": "Please enter your answer",
            "description": null,
            "isRequired": false,
            "relativeOrder": 1
          }
        ]
      },
      {
        "attributeSetId": 535576,
        "attributeSetTypeId": 500008,
        "attributeSetRelationTypeId": 500005,
        "attributeSetName": "There are four types of applications currently supported on the Neocortex system, divided into the following individual tracks:\n\n    Track 1: Cerebras modelzoo ML models\n    Track 2: Models similar to the Cerebras modelzoo models\n    Track 3: General purpose SDK\n    Track 4: WFA, WSE Field-equation API\n\nPlease see:\n\n https://portal.neocortex.psc.edu/docs/index.html\n\nfor further details.",
        "relativeOrder": 3,
        "isActive": true,
        "attributes": [
          {
            "resourceAttributeId": 536130,
            "attributeName": "Track 1: Cerebras modelzoo ML models",
            "description": null,
            "isRequired": true,
            "relativeOrder": 1
          },
          {
            "resourceAttributeId": 536131,
            "attributeName": "Track 2: Models similar to the Cerebras modelzoo models",
            "description": null,
            "isRequired": true,
            "relativeOrder": 2
          },
          {
            "resourceAttributeId": 536132,
            "attributeName": "Track 3: General purpose SDK",
            "description": null,
            "isRequired": true,
            "relativeOrder": 3
          },
          {
            "resourceAttributeId": 536133,
            "attributeName": "Track 4: WFA, WSE Field-equation API",
            "description": null,
            "isRequired": true,
            "relativeOrder": 4
          }
        ]
      },
      {
        "attributeSetId": 536040,
        "attributeSetTypeId": 500007,
        "attributeSetRelationTypeId": 500006,
        "attributeSetName": "Did you address all the questions associated with the track(s) in an attached application PDF document? (*). The track specific questions are available at https://portal.neocortex.psc.edu/docs/index.html. Failure to address these questions may result in your submission being rejected without review.",
        "relativeOrder": 4,
        "isActive": true,
        "attributes": [
          {
            "resourceAttributeId": 536396,
            "attributeName": "Please enter your answer",
            "description": null,
            "isRequired": false,
            "relativeOrder": 1
          }
        ]
      }
    ],
    "featureCategories": [
      {
        "categoryId": 100,
        "categoryName": "Resource Type",
        "categoryDescription": "The general kind of resource",
        "categoryIsFilter": true,
        "features": [
          {
            "featureId": 135,
            "name": "Innovative / Novel Compute",
            "description": "Unique, innovative or non-traditional compute resource",
            "isFilter": null
          }
        ]
      },
      {
        "categoryId": 101,
        "categoryName": "Allocations",
        "categoryDescription": "Whether resource is allocated by ACCESS or not",
        "categoryIsFilter": true,
        "features": [
          {
            "featureId": 140,
            "name": "RP-managed process",
            "description": "Resource is not allocated by ACCESS",
            "isFilter": null
          }
        ]
      },
      {
        "categoryId": 133,
        "categoryName": "Resource Category",
        "categoryDescription": "NAIRR Pilot Resource Category",
        "categoryIsFilter": true,
        "features": [
          {
            "featureId": 232,
            "name": "Federal agency systems",
            "description": "Agency supercomputers and advanced architecture systems",
            "isFilter": null
          }
        ]
      }
    ]
  },
  {
    "resourceId": 530770,
    "resourceName": "Purdue Anvil CPU",
    "displayResourceName": "Purdue Anvil CPU",
    "resourceType": "Compute",
    "description": "1 Anvil CPU Service Unit = 1 Core Hour on a regular node\r\n4 Anvil CPU Service Units = 1 Core Hour on a large memory node\r\n",
    "units": "SUs",
    "organization": "Purdue University",
    "resourceRepositoryKey": 145411,
    "productionBeginDate": "2021-10-01",
    "productionEndDate": "2026-10-01",
    "resourceDescription": "Purdue's Anvil cluster built in partnership with Dell and AMD consists of 1,000 nodes with two 64-core AMD EPYC \"Milan\" processors each and delivers over 1 billion CPU core hours each year, with a peak performance of 5.1 petaflops. Each of these nodes has 256GB of DDR4-3200 memory. A separate set of 32 large memory nodes has 1TB of DDR4-3200 memory each. Anvil's nodes are interconnected with 100 Gbps Mellanox HDR100 InfiniBand.",
    "recommendedUse": "Anvil's general-purpose CPUs and 128 cores per node make it suitable for many types of CPU-based workloads including the most common modeling and simulation codes across all science and engineering domains. Memory-intensive workloads can be executed on the large memory nodes on Anvil. Anvil’s Composable Subsystem which is a Kubernetes based private cloud consists of both CPU and GPU nodes and large (S3) data storage. The Composable Subsystem is suitable for applications such as model inference service (via NVIDIA Triton), Specialized LLMs, dataset hosting, science gateways and web application hosting, and classroom and training applications via interactive access interfaces.",
    "conversionFactor": "9.6174",
    "userGuideUrl": "https://www.rcac.purdue.edu/anvil#docs",
    "attributeSets": [],
    "featureCategories": [
      {
        "categoryId": 103,
        "categoryName": "Specialized Support",
        "categoryDescription": "Special services offered by the resource",
        "categoryIsFilter": true,
        "features": [
          {
            "featureId": 166,
            "name": "ACCESS Pegasus",
            "description": "Run Jobs and Workflows on ACCESS Resources from a Single Entry Point",
            "isFilter": null
          },
          {
            "featureId": 167,
            "name": "ACCESS OnDemand",
            "description": "An intuitive, innovative, and interactive interface to remote computing resources",
            "isFilter": null
          },
          {
            "featureId": 146,
            "name": "Science Gateway support",
            "description": "Support for Science Gateways",
            "isFilter": null
          }
        ]
      },
      {
        "categoryId": 101,
        "categoryName": "Allocations",
        "categoryDescription": "Whether resource is allocated by ACCESS or not",
        "categoryIsFilter": true,
        "features": [
          {
            "featureId": 139,
            "name": "ACCESS Allocated",
            "description": "Resource is allocated by ACCESS",
            "isFilter": null
          }
        ]
      },
      {
        "categoryId": 102,
        "categoryName": "Specialized Hardware",
        "categoryDescription": "Key compute hardware components in this resource",
        "categoryIsFilter": true,
        "features": [
          {
            "featureId": 142,
            "name": "Large Memory Nodes",
            "description": "Specialized analysis nodes with large memory",
            "isFilter": null
          }
        ]
      },
      {
        "categoryId": 100,
        "categoryName": "Resource Type",
        "categoryDescription": "The general kind of resource",
        "categoryIsFilter": true,
        "features": [
          {
            "featureId": 136,
            "name": "CPU Compute",
            "description": "General compute use",
            "isFilter": null
          }
        ]
      },
      {
        "categoryId": 133,
        "categoryName": "Resource Category",
        "categoryDescription": "NAIRR Pilot Resource Category",
        "categoryIsFilter": true,
        "features": [
          {
            "featureId": 232,
            "name": "Federal agency systems",
            "description": "Agency supercomputers and advanced architecture systems",
            "isFilter": null
          }
        ]
      }
    ]
  },
  {
    "resourceId": 530574,
    "resourceName": "Kentucky Research Informatics Cloud (KyRIC) Large Memory Nodes",
    "displayResourceName": "Kentucky Research Informatics Cloud (KyRIC) Large Memory Nodes",
    "resourceType": "Compute",
    "description": "Each Core hr is 1 CPU core with a default memory of 75GB RAM/core",
    "units": "Core-hours",
    "organization": "University of Kentucky",
    "resourceRepositoryKey": 145213,
    "productionBeginDate": "2021-04-01",
    "productionEndDate": "2025-03-31",
    "resourceDescription": "Five large memory compute nodes dedicated for XSEDE allocation. Each of these nodes have 40 cores (Broadwell class and lntel(R) Xeon(R) CPU E7-4820 v4 @ 2.00GHz with 4 sockets, 10 cores/socket), 3TB RAM, and 6TB SSD storage drives. The 5 dedicated XSEDE nodes will have exclusive access to approximately 300 TB of network attached disk storage. All these compute nodes are interconnected through a 100 Gigabit Ethernet (l00GbE) backbone and the cluster login and data transfer nodes will be connected through a 100Gb uplink to lnternet2 for external connections.",
    "recommendedUse": "Large memory nodes are increasingly needed by a wide range of XSEDE researchers, particularly researchers working with big data such as massive NLP data sets used in many research domains or the massive genomes required by computational biology and bioinformatics.",
    "conversionFactor": "8.01",
    "userGuideUrl": "https://xsedetoaccess.ccs.uky.edu/confluence/redirect/KyRIC+Kentucky.html",
    "attributeSets": [],
    "featureCategories": [
      {
        "categoryId": 100,
        "categoryName": "Resource Type",
        "categoryDescription": "The general kind of resource",
        "categoryIsFilter": true,
        "features": [
          {
            "featureId": 136,
            "name": "CPU Compute",
            "description": "General compute use",
            "isFilter": null
          }
        ]
      },
      {
        "categoryId": 101,
        "categoryName": "Allocations",
        "categoryDescription": "Whether resource is allocated by ACCESS or not",
        "categoryIsFilter": true,
        "features": [
          {
            "featureId": 139,
            "name": "ACCESS Allocated",
            "description": "Resource is allocated by ACCESS",
            "isFilter": null
          }
        ]
      },
      {
        "categoryId": 102,
        "categoryName": "Specialized Hardware",
        "categoryDescription": "Key compute hardware components in this resource",
        "categoryIsFilter": true,
        "features": [
          {
            "featureId": 142,
            "name": "Large Memory Nodes",
            "description": "Specialized analysis nodes with large memory",
            "isFilter": null
          }
        ]
      },
      {
        "categoryId": 103,
        "categoryName": "Specialized Support",
        "categoryDescription": "Special services offered by the resource",
        "categoryIsFilter": true,
        "features": [
          {
            "featureId": 167,
            "name": "ACCESS OnDemand",
            "description": "An intuitive, innovative, and interactive interface to remote computing resources",
            "isFilter": null
          }
        ]
      }
    ]
  },
  {
    "resourceId": 532123,
    "resourceName": "PSC Neocortex Superdome Flex Server",
    "displayResourceName": "PSC Neocortex Superdome Flex Server",
    "resourceType": "Compute",
    "description": "",
    "units": "Core-hours",
    "organization": "Pittsburgh Supercomputing Center",
    "resourceRepositoryKey": 147026,
    "productionBeginDate": "2024-04-15",
    "productionEndDate": null,
    "resourceDescription": "Neocortex is a highly innovative resource that targets the acceleration of AI-powered scientific discovery by vastly shortening the time required for deep learning training, fostering greater integration of artificial deep learning with scientific workflows, and providing revolutionary new hardware for the development of more efficient algorithms for artificial intelligence and high performance computing. \r\n\r\nThe HPE Superdome Flex (SDFlex) features 32 Intel Xeon Platinum 8280L CPUs with 28 cores (56 threads) each, 2.70-4.0 GHz, 38.5 MB cache, 24 TiB RAM, aggregate memory bandwidth of 4.5 TB/s, and 204.6 TB aggregate local storage capacity with 150 GB/s read bandwidth. The SDF can provide 1.2 Tb/s to each CS-2 system and 1.6 Tb/s from the Bridges-2 filesystems.",
    "recommendedUse": null,
    "conversionFactor": "80.145",
    "userGuideUrl": "portal.neocortex.psc.edu",
    "attributeSets": [],
    "featureCategories": [
      {
        "categoryId": 101,
        "categoryName": "Allocations",
        "categoryDescription": "Whether resource is allocated by ACCESS or not",
        "categoryIsFilter": true,
        "features": [
          {
            "featureId": 139,
            "name": "ACCESS Allocated",
            "description": "Resource is allocated by ACCESS",
            "isFilter": null
          }
        ]
      },
      {
        "categoryId": 133,
        "categoryName": "Resource Category",
        "categoryDescription": "NAIRR Pilot Resource Category",
        "categoryIsFilter": true,
        "features": [
          {
            "featureId": 232,
            "name": "Federal agency systems",
            "description": "Agency supercomputers and advanced architecture systems",
            "isFilter": null
          }
        ]
      },
      {
        "categoryId": 100,
        "categoryName": "Resource Type",
        "categoryDescription": "The general kind of resource",
        "categoryIsFilter": true,
        "features": [
          {
            "featureId": 135,
            "name": "Innovative / Novel Compute",
            "description": "Unique, innovative or non-traditional compute resource",
            "isFilter": null
          }
        ]
      }
    ]
  },
  {
    "resourceId": 530204,
    "resourceName": "TACC Dell/Intel Knights Landing, Skylake System (Stampede2)",
    "displayResourceName": "TACC Dell/Intel Knights Landing, Skylake System (Stampede2)",
    "resourceType": "Compute",
    "description": "Stampede 2 is allocated in service units (SU)s. An SU is defined as 1 wall-clock node hour. Allocations awarded on Stampede2 may be used on either or both node types.",
    "units": "Node Hours",
    "organization": "Texas Advanced Computing Center",
    "resourceRepositoryKey": 144584,
    "productionBeginDate": "2017-06-13",
    "productionEndDate": "2023-09-30",
    "resourceDescription": "PLEASE NOTE THAT STAMPEDE2 WILL NO LONGER BE AVAILABLE AS OF JULY 1, 2023.  If you require larger scale resources you should consider the leadership computing capabilities of Frontera https://frontera-portal.tacc.utexas.edu/allocations/  Otherwise we are currently recommending PI's who are new to ACCESS to consider other resources.\r\n\r\nThe Stampede2 Dell/Intel Knights Landing (KNL), Skylake (SKX) System provides the user community access to two Intel Xeon compute technologies.\r\n\r\nThe system is configured with 4204 Dell KNL compute nodes, each with a stand-alone Intel Xeon Phi Knights Landing bootable processor.  Each KNL node includes 68 cores, 16GB MCDRAM, 96GB DDR-4 memory and a 200GB SSD drive.  \r\n\r\nStampede2 also includes 1736 Intel Xeon Skylake (SKX) nodes and additional management nodes. Each SKX includes 48 cores, 192GB DDR-4 memory, and a 200GB SSD.\r\n\r\nAllocations awarded on Stampede2 may be used on either or both of the node types.\r\n\r\nCompute nodes have access to dedicated Lustre Parallel file systems totaling 28PB raw, provided by Cray. An Intel Omni-Path Architecture switch fabric connects the nodes and storage through a fat-tree topology with a point to point bandwidth of 100 Gb/s (unidirectional speed). 16 additional login and management servers complete the system. Stampede2 will deliver an estimated 18PF of peak performance.\r\n\r\nPlease see the Stampede2 User Guide for detailed information on the system and how to most effectively use it.\r\n\r\nhttps://portal.tacc.utexas.edu/user-guides/stampede2",
    "recommendedUse": "Stampede2 is intended primarily for parallel applications scalable to tens of thousands of cores, as well as general purpose and throughput computing. Normal batch queues will enable users to run simulations up to 48 hours. Jobs requiring run times and more cores than allowed by the normal queues will be run in a special queue after approval of TACC staff. normal, serial and development queues are configured as well as special purpose queues.",
    "conversionFactor": "143.719",
    "userGuideUrl": "https://docs.tacc.utexas.edu/hpc/stampede2/",
    "attributeSets": [],
    "featureCategories": []
  }
]
